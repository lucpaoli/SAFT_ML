{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/SAFT_ML`\n"
     ]
    }
   ],
   "source": [
    "import Pkg; Pkg.activate(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CUDA\n",
    "using Flux\n",
    "using Flux: onehotbatch, onecold\n",
    "using Flux.Losses: logitcrossentropy\n",
    "using Flux.Data: DataLoader\n",
    "using GeometricFlux\n",
    "using GeometricFlux.Datasets\n",
    "using GraphSignals\n",
    "using Graphs\n",
    "using Parameters: @with_kw\n",
    "using ProgressMeter: Progress, next!\n",
    "using Statistics\n",
    "using Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function load_data(dataset, batch_size, train_repeats=32, test_repeats=2)\n",
    "    s, t = dataset[1].edge_index\n",
    "    g = Graphs.Graph(dataset[1].num_nodes)\n",
    "    for (i, j) in zip(s, t)\n",
    "        Graphs.add_edge!(g, i, j)\n",
    "    end\n",
    "\n",
    "    data = dataset[1].node_data\n",
    "    X, y = data.features, onehotbatch(data.targets, 1:7)\n",
    "    # return dataset\n",
    "    # return X, y\n",
    "    train_idx, test_idx = data.train_mask, data.val_mask\n",
    "\n",
    "    # (train_X, train_y) dim: (num_features, target_dim) × 2708 × train_repeats\n",
    "    train_X, train_y = repeat(X, outer=(1,1,train_repeats)), repeat(y, outer=(1,1,train_repeats))\n",
    "    # (test_X, test_y) dim: (num_features, target_dim) × 2708 × test_repeats\n",
    "    test_X, test_y = repeat(X, outer=(1,1,test_repeats)), repeat(y, outer=(1,1,test_repeats))\n",
    "\n",
    "    add_all_self_loops!(g)\n",
    "    fg = FeaturedGraph(g)\n",
    "    train_loader = DataLoader((train_X, train_y), batchsize=batch_size, shuffle=true)\n",
    "    test_loader = DataLoader((test_X, test_y), batchsize=batch_size, shuffle=true)\n",
    "    # return train_loader, test_loader, fg, train_idx, test_idx\n",
    "    return test_X, test_y, fg\n",
    "end\n",
    "\n",
    "function add_all_self_loops!(g)\n",
    "    for i in vertices(g)\n",
    "        add_edge!(g, i, i)\n",
    "    end\n",
    "    return g\n",
    "end\n",
    "\n",
    "@with_kw mutable struct Args\n",
    "    η = 0.01                # learning rate\n",
    "    batch_size = 8          # batch size\n",
    "    epochs = 20             # number of epochs\n",
    "    seed = 0                # random seed\n",
    "    cuda = false            # use GPU\n",
    "    heads = 2               # attention heads\n",
    "    input_dim = 1433        # input dimension\n",
    "    hidden_dim = 16         # hidden dimension\n",
    "    target_dim = 7          # target dimension\n",
    "    dataset = Cora          # dataset to train on\n",
    "end\n",
    "\n",
    "# Cora dataset has:\n",
    "# - 2708 nodes\n",
    "# - 1433 features\n",
    "# - 7 classes\n",
    "# - 10556 edges\n",
    "\n",
    "# Nodes represent documents, edges represent citations.\n",
    "\n",
    "## Loss: cross entropy\n",
    "model_loss(model, X, y, idx) =\n",
    "    logitcrossentropy(model(X)[:,idx,:], y[:,idx,:])\n",
    "\n",
    "accuracy(model, X::AbstractArray, y::AbstractArray, idx) =\n",
    "    mean(onecold(softmax(cpu(model(X))[:,idx,:])) .== onecold(cpu(y)[:,idx,:]))\n",
    "\n",
    "accuracy(model, loader::DataLoader, device, idx) =\n",
    "    mean(accuracy(model, X |> device, y |> device, idx) for (X, y) in loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0;;; 0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0], FeaturedGraph:\n",
       "\tUndirected graph with (#V=2708, #E=7986) in adjacency matrix)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "args = Args()\n",
    "# train_loader, test_loader, fg, train_idx, test_idx = load_data(args.dataset(), args.batch_size)\n",
    "test_X, test_y, fg = load_data(args.dataset(), args.batch_size)\n",
    "# args = Args()\n",
    "# args.seed > 0 && Random.seed!(args.seed)\n",
    "# X, y = load_data(args.dataset(), args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training on CPU\n",
      "└ @ Main /home/luc/SAFT_ML/3_gnn_example.ipynb:66\n",
      "┌ Info: Data loaded, building model...\n",
      "└ @ Main /home/luc/SAFT_ML/3_gnn_example.ipynb:72\n"
     ]
    }
   ],
   "source": [
    "\n",
    "function train(; kws...)\n",
    "    # load hyperparamters\n",
    "    args = Args(; kws...)\n",
    "    args.seed > 0 && Random.seed!(args.seed)\n",
    "\n",
    "    # GPU config\n",
    "    if args.cuda && CUDA.has_cuda()\n",
    "        device = gpu\n",
    "        CUDA.allowscalar(false)\n",
    "        @info \"Training on GPU\"\n",
    "    else\n",
    "        device = cpu\n",
    "        @info \"Training on CPU\"\n",
    "    end\n",
    "\n",
    "    # load Cora from Planetoid dataset\n",
    "    train_loader, test_loader, fg, train_idx, test_idx = load_data(args.dataset(), args.batch_size)\n",
    "\n",
    "    @info \"Data loaded, building model...\"\n",
    "\n",
    "    # build model\n",
    "    model = Chain(\n",
    "        WithGraph(fg, GATConv(args.input_dim => args.hidden_dim, heads=args.heads)),\n",
    "        Dropout(0.6),\n",
    "        WithGraph(fg, GATConv(args.hidden_dim * args.heads => args.target_dim, heads=args.heads, concat=false)),\n",
    "    ) |> device\n",
    "\n",
    "    @info \"Model built, loading optimiser and parameters...\"\n",
    "\n",
    "    # Adam optimizer\n",
    "    opt = Adam(args.η)\n",
    "\n",
    "    # parameters\n",
    "    ps = Flux.params(model)\n",
    "\n",
    "    # training\n",
    "    train_steps = 0\n",
    "    @info \"Starting Training, total $(args.epochs) epochs\"\n",
    "    for epoch = 1:args.epochs\n",
    "        @info \"Epoch $(epoch)\"\n",
    "        progress = Progress(length(train_loader))\n",
    "\n",
    "        for (X, y) in train_loader\n",
    "            X, y, device_idx = X |> device, y |> device, train_idx |> device\n",
    "            loss, back = Flux.pullback(() -> model_loss(model, X, y, device_idx), ps)\n",
    "            train_acc = accuracy(model, train_loader, device, train_idx)\n",
    "            test_acc = accuracy(model, test_loader, device, test_idx)\n",
    "            grad = back(1.0f0)\n",
    "            Flux.Optimise.update!(opt, ps, grad)\n",
    "\n",
    "            # progress meter\n",
    "            next!(progress; showvalues=[\n",
    "                (:loss, loss),\n",
    "                (:train_accuracy, train_acc),\n",
    "                (:test_accuracy, test_acc)\n",
    "            ])\n",
    "\n",
    "            train_steps += 1\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return model, args\n",
    "end\n",
    "\n",
    "model, args = train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
