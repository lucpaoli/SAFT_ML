{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/SAFT_ML`\n"
     ]
    }
   ],
   "source": [
    "import Pkg; Pkg.activate(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: using Graphs.Δ in module Main conflicts with an existing identifier.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefinition of constant UNIT_FORMATS. This may fail, cause incorrect answers, or produce other errors.\n"
     ]
    }
   ],
   "source": [
    "using Clapeyron\n",
    "includet(\"./saftvrmienn.jl\")\n",
    "import Clapeyron: a_res\n",
    "\n",
    "using MolecularGraph, Graphs\n",
    "using Plots\n",
    "\n",
    "using Flux\n",
    "# using Flux: onecold, onehotbatch, logitcrossentropy\n",
    "using Flux: DataLoader\n",
    "using GraphNeuralNetworks\n",
    "using ForwardDiff, Zygote, ChainRulesCore\n",
    "\n",
    "using MLUtils\n",
    "using OneHotArrays\n",
    "# using LinearAlgebra, Random, Statistics\n",
    "using Statistics, Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# atom_symbol(mol): atom letters as a symbol e.g. :C, :O and :N\n",
    "# charge(mol): electric charge of the atom. only integer charge is allowed in the model\n",
    "# multiplicity(mol): 1: no unpaired electron(default), 2: radical, 3: biradical\n",
    "# lone_pair(mol): number of lone pair on the atom\n",
    "# implicit_hydrogens(mol): number of implicit hydrogens that are not appear as graph vertices but automatically calculated, drawn in image and used for calculation of other descriptors.\n",
    "# valence(mol): number of atom valence, specific to each atom species and considering electric charge. Implicit number of hydrogens is obtained by subtracting the degree of the vertex from the valence.\n",
    "# is_aromatic(mol): whether the atom is aromatic or not. only binary aromaticity is allowed in the model.\n",
    "# pi_electron(mol): number of pi electrons\n",
    "# hybridization(mol): orbital hybridization e.g. sp, sp2 and sp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GNNGraph:\n",
       "  num_nodes: 1\n",
       "  num_edges: 0\n",
       "  ndata:\n",
       "\tx = 11×1 Matrix{Float32}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function make_graph_from_smiles(smiles::String)\n",
    "    molgraph = smilestomol(smiles)\n",
    "\n",
    "    g = SimpleGraph(nv(molgraph))\n",
    "    for e in edges(molgraph)\n",
    "        add_edge!(g, e.src, e.dst)\n",
    "    end\n",
    "\n",
    "    # Should number of hydrogens be one-hot encoded?\n",
    "    f(vec, enc) = hcat(map(x -> onehot(x, enc), vec)...)\n",
    "    num_h = f(implicit_hydrogens(molgraph), [0, 1, 2, 3, 4])\n",
    "    hybrid = f(hybridization(molgraph), [:sp, :sp2, :sp3])\n",
    "    atoms = f(atom_symbol(molgraph), [:C, :O, :N])\n",
    "\n",
    "    # Node data should be matrix (num_features, num_nodes)\n",
    "    # Matrix has num_nodes columns, num_features rows\n",
    "    ndata = Float32.(vcat(num_h, hybrid, atoms))\n",
    "\n",
    "    # h(vec, enc) = hcat(map(x -> onehot(x, enc), vec)...)\n",
    "    # @show bond_order(molgraph), is_rotatable(molgraph), is_aromatic(molgraph), collect(edges(molgraph))\n",
    "    b_order = Float32.(f(bond_order(molgraph), [1, 2, 3]))\n",
    "    # @show b_order\n",
    "    # rotatable = f(is_rotatable(molgraph), [false, true])\n",
    "    # edata = Matrix{Float32}(vcat(b_order, rotatable))\n",
    "    # edata = Matrix{Float32}(b_order)\n",
    "    edata = nothing\n",
    "    \n",
    "    g = GNNGraph(g, ndata = ndata, edata = edata)\n",
    "    return g\n",
    "end\n",
    "\n",
    "g = make_graph_from_smiles(\"C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.04"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = SAFTVRMie([\"methane\"])\n",
    "fieldnames(typeof(model.params.Mw))\n",
    "model.params.Mw.values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: NaN found in V_vec at T = 627.610279365275 for decane\n",
      "└ @ Main /home/luc/SAFT_ML/4_gnn_saft.ipynb:69\n"
     ]
    }
   ],
   "source": [
    "# Iterate over molecules in dataset and build graph for each one\n",
    "# Initially sample data for hydrocarbons\n",
    "#! isobutane, isopentane not defined for SAFTVRMie\n",
    "all_species = [\n",
    "    \"methane\",\n",
    "    \"ethane\",\n",
    "    \"propane\",\n",
    "    \"butane\",\n",
    "    \"pentane\",\n",
    "    \"hexane\",\n",
    "    \"heptane\",\n",
    "    \"octane\",\n",
    "    \"nonane\",\n",
    "    \"decane\",\n",
    "]\n",
    "\n",
    "# Define smiles map\n",
    "smiles_map = Dict(\n",
    "    \"methane\" => \"C\",\n",
    "    \"ethane\" => \"CC\",\n",
    "    \"propane\" => \"CCC\",\n",
    "    \"butane\" => \"CCCC\",\n",
    "    \"isobutane\" => \"CC(C)C\",\n",
    "    \"pentane\" => \"CCCCC\",\n",
    "    \"isopentane\" => \"CC(C)CC\",\n",
    "    \"hexane\" => \"CCCCCC\",\n",
    "    \"heptane\" => \"CCCCCCC\",\n",
    "    \"octane\" => \"CCCCCCCC\",\n",
    "    \"nonane\" => \"CCCCCCCCC\",\n",
    "    \"decane\" => \"CCCCCCCCCC\",\n",
    ")\n",
    "\n",
    "# Create training data, currently sampled along saturation curve\n",
    "\n",
    "T = GNNGraph{Tuple{Vector{Int64}, Vector{Int64}, Nothing}}\n",
    "graphs = T[]\n",
    "states = Vector{Float32}[]\n",
    "species = String[] # For checking parameter similarity\n",
    "Y_data = Float32[]\n",
    "\n",
    "n = 15\n",
    "for s in all_species\n",
    "    # model = GERG2008([s])\n",
    "    model = SAFTVRMie([s])\n",
    "    Tc, pc, Vc = crit_pure(model)\n",
    "    smiles = smiles_map[s]\n",
    "\n",
    "    # fingerprint = make_fingerprint(smiles)\n",
    "    g = make_graph_from_smiles(smiles)\n",
    "\n",
    "    T_range = range(0.5 * Tc, 0.99 * Tc, n)\n",
    "    # V_range = range(0.5 * Vc, 1.5 * Vc, n) # V could be sampled from a logspace\n",
    "    for T in T_range\n",
    "        (p₀, V_vec...) = saturation_pressure(model, T)\n",
    "        if !any(isnan.(V_vec))\n",
    "            for V in V_vec\n",
    "                push!(graphs, g)\n",
    "                push!(species, s)\n",
    "\n",
    "                Mw = model.params.Mw.values[1]\n",
    "                m = model.params.segment.values[1]\n",
    "                push!(states, [V, T, Mw, m])\n",
    "\n",
    "                a = a_res(model, V, T, [1.0])\n",
    "                @assert !isnan(a) \"a is NaN at (V,T) = ($(V),$(T)) for $s\"\n",
    "                push!(Y_data, a)\n",
    "            end\n",
    "        else\n",
    "            @warn \"NaN found in V_vec at T = $T for $s\"\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: generating data for 80 molecules\n",
      "└ @ Main /home/luc/SAFT_ML/4_gnn_saft.ipynb:7\n"
     ]
    }
   ],
   "source": [
    "using CSV, DataFrames\n",
    "\n",
    "# Create training & validation data\n",
    "df = CSV.read(\"./pcpsaft_params/SI_pcp-saft_parameters.csv\", DataFrame, header=1)\n",
    "filter!(row -> occursin(\"Alkane\", row.family), df)\n",
    "mol_data = zip(df.common_name, df.isomeric_smiles, df.molarweight)\n",
    "@info \"generating data for $(length(mol_data)) molecules\"\n",
    "\n",
    "# Create training data, currently sampled along saturation curve\n",
    "T = GNNGraph{Tuple{Vector{Int64}, Vector{Int64}, Nothing}}\n",
    "graphs = T[]\n",
    "states = Vector{Float32}[]\n",
    "species = String[] # For checking parameter similarity\n",
    "Y_data = Float32[]\n",
    "\n",
    "n = 15\n",
    "# for s in all_species\n",
    "for (name, smiles, Mw) in mol_data\n",
    "    saft_model = PPCSAFT([name])\n",
    "    Tc, pc, Vc = crit_pure(model)\n",
    "\n",
    "    # fingerprint = make_fingerprint(smiles)\n",
    "    g = make_graph_from_smiles(smiles)\n",
    "\n",
    "    T_range = range(0.5 * Tc, 0.99 * Tc, n)\n",
    "    # V_range = range(0.5 * Vc, 1.5 * Vc, n) # V could be sampled from a logspace\n",
    "    for T in T_range\n",
    "        (p₀, V_vec...) = saturation_pressure(model, T)\n",
    "        if !any(isnan.(V_vec))\n",
    "            for V in V_vec\n",
    "                push!(graphs, g)\n",
    "                push!(species, name)\n",
    "\n",
    "                # Mw = model.params.Mw.values[1]\n",
    "                # m = model.params.segment.values[1]\n",
    "                push!(states, [V, T, Mw])\n",
    "\n",
    "                a = a_res(model, V, T, [1.0])\n",
    "                @assert !isnan(a) \"a is NaN at (V,T) = ($(V),$(T)) for $name\"\n",
    "                push!(Y_data, a)\n",
    "            end\n",
    "        else\n",
    "            @warn \"NaN found in V_vec at T = $T for $name\"\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(630.7620907026924, 2.2684498427389706e6, 0.0006551521374012365)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = SAFTgammaMie([\"decane\"])\n",
    "crit_pure(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GNNGraph:\n",
       "  num_nodes: 4\n",
       "  num_edges: 6\n",
       "  ndata:\n",
       "\tx = 11×4 Matrix{Float32}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3-element Vector{Float32}:\n",
       "   0.0036027166\n",
       " 125.08929\n",
       "  58.078"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "-0.058258507f0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 10\n",
    "display(graphs[n])\n",
    "display(states[n])\n",
    "display(Y_data[n])\n",
    "nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GNNGraph:\n",
       "  num_nodes: 295\n",
       "  num_edges: 526\n",
       "  num_graphs: 32\n",
       "  ndata:\n",
       "\tx = 11×295 Matrix{Float32}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data, test_data = splitobs((graphs, states, species, Y_data), at = 0.8, shuffle = true) |> getobs\n",
    "\n",
    "Random.seed!(0)\n",
    "train_loader = DataLoader(train_data, batchsize = 32, shuffle = true)\n",
    "test_loader = DataLoader(test_data, batchsize = 32, shuffle = false)\n",
    "\n",
    "# Testing if batching works. This will be used when training\n",
    "# This should produce a single GNNGraph object with a matrix of ndata\n",
    "vec_gs, _ = first(train_loader)\n",
    "MLUtils.batch(vec_gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "differentiable_saft (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function differentiable_saft(X::AbstractVector{T}, Vol, Temp, Mw, m) where {T<:Real}\n",
    "    model = SAFTVRMieNN(\n",
    "        params=SAFTVRMieNNParams(\n",
    "            Mw=[Mw],\n",
    "            segment=[m], # (C - 4)/(3) + 1\n",
    "            sigma=[X[1]] * 1f-10,\n",
    "            lambda_a=[6.0], # Fixing at 6. Simple molecules interacting through London dispersion -> Should have λₐ = 6.\n",
    "            lambda_r=[X[2]],\n",
    "            epsilon=[X[3]],\n",
    "            # Required for association\n",
    "            epsilon_assoc=Float32[],\n",
    "            bondvol=Float32[],\n",
    "        )\n",
    "    )\n",
    "    return a_res(model, Vol, Temp, [1.0])\n",
    "end\n",
    "\n",
    "# function ChainRulesCore.rrule(::typeof(differentiable_saft), x, V, T, Mw, m)\n",
    "#     y = differentiable_saft(x, V, T, Mw, m)\n",
    "\n",
    "#     function f_pullback(Δy)\n",
    "#         # Use ForwardDiff to compute the gradient\n",
    "#         #? ForwardDiff through nonlinear SAFT solvers not ideal.\n",
    "#         ∂x = @thunk(ForwardDiff.gradient(x -> differentiable_saft(x, V, T, Mw, m), x) .* Δy)\n",
    "#         ∂V = @thunk(ForwardDiff.derivative(V -> differentiable_saft(x, V, T, Mw, m), V) * Δy)\n",
    "#         ∂T = @thunk(ForwardDiff.derivative(T -> differentiable_saft(x, V, T, Mw, m), T) * Δy)\n",
    "#         ∂Mw = @thunk(ForwardDiff.derivative(Mw -> differentiable_saft(x, V, T, Mw, m), Mw) * Δy)\n",
    "#         ∂m = @thunk(ForwardDiff.derivative(m -> differentiable_saft(x, V, T, Mw, m), m) * Δy)\n",
    "#         return (NoTangent(), ∂x, ∂V, ∂T, ∂Mw, ∂m)\n",
    "#     end\n",
    "\n",
    "#     return y, f_pullback\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differentiable_saft(X, 0.0001, 300, 14.0, 1.0) = -0.28273695216408445\n",
      "ForwardDiff.gradient((x->begin\n",
      "            #= /home/luc/SAFT_ML/4_gnn_saft.ipynb:4 =#\n",
      "            differentiable_saft(x, 0.0001, 300, 14.0, 1.0)\n",
      "        end), X) = "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.12753186189116184, 0.05384652383064771, -0.006372734115166813]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zygote.gradient((x->begin\n",
      "            #= /home/luc/SAFT_ML/4_gnn_saft.ipynb:5 =#\n",
      "            differentiable_saft(x, 0.0001, 300, 14.0, 1.0)\n",
      "        end), X) = ([-0.12753186189116192, 0.053846523830647745, -0.006372734115166815],)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-0.12753186189116192, 0.053846523830647745, -0.006372734115166815],)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#? Works for evaluation, fails for gradients\n",
    "X = [3.737, 12.504, 152.58]\n",
    "@show differentiable_saft(X, 1e-4, 300, 14.0, 1.0)\n",
    "@show ForwardDiff.gradient(x -> differentiable_saft(x, 1e-4, 300, 14.0, 1.0), X)\n",
    "@show Zygote.gradient(x -> differentiable_saft(x, 1e-4, 300, 14.0, 1.0), X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function create_graphconv_model(nin, nh; nout=3, nhlayers=1, afunc=relu)\n",
    "    GNNChain(\n",
    "        GraphConv(nin => nh, afunc),\n",
    "        [GraphConv(nh => nh, afunc) for _ in 1:nhlayers]...,\n",
    "        GlobalPool(mean), # Average the node features\n",
    "        Dropout(0.2),\n",
    "        Dense(nh, nh),\n",
    "        Dense(nh, nout),\n",
    "    )\n",
    "end\n",
    "\n",
    "function create_graphattention_model(nin, ein, nh; nout=3, nhlayers=1, afunc=relu)\n",
    "    GNNChain(\n",
    "        GATv2Conv((nin, ein) => nh, afunc),\n",
    "        [GATv2Conv(nh => nh, afunc) for _ in 1:nhlayers]...,\n",
    "        GlobalPool(mean),\n",
    "        Dropout(0.2),\n",
    "        Dense(nh, nh),\n",
    "        Dense(nh, nout),\n",
    "    )\n",
    "end\n",
    "\n",
    "function bound_output(X, lb, ub, b=10.0)\n",
    "    return @. lb + (ub - lb) * 0.5 * (tanh(1 / b * (X - lb) / (ub - lb)) + 1)\n",
    "end\n",
    "\n",
    "function predict_a_res(X, V, T, Mw, m)\n",
    "    # Bound output\n",
    "    bounds = Tuple{Float32,Float32}[\n",
    "        (2.5, 5), # σ\n",
    "        (10, 20), # λ_r\n",
    "        (100, 1000), # ϵ #? What should this be bounded to for SAFTVRMie?\n",
    "    ]\n",
    "    # ŷ = mean(X)\n",
    "    X_bounded = bound_output(X, first.(bounds), last.(bounds))\n",
    "    ŷ = differentiable_saft(X_bounded, V, T, Mw, m)\n",
    "    return ŷ + mean(X)\n",
    "end\n",
    "\n",
    "function eval_loss(model, data_loader, device)\n",
    "    loss = 0.0\n",
    "    acc = 0.0\n",
    "    for (g, state, species, y) in data_loader\n",
    "        g, state, y = MLUtils.batch(g) |> device, state |> device, y |> device\n",
    "        X = model(g, g.ndata.x)\n",
    "        for (Xᵢ, stateᵢ, yᵢ) in zip(eachcol(X), state, y)\n",
    "            V, T, Mw, m = stateᵢ\n",
    "            ŷ = predict_a_res(Xᵢ, V, T, Mw, m)\n",
    "            loss += ((ŷ - yᵢ) / yᵢ)^2\n",
    "            acc += abs((ŷ - yᵢ) / yᵢ)\n",
    "            @assert loss isa Real \"Loss is not a real number, got $(typeof(loss)), X_pred = $X_pred\"\n",
    "            @assert !isnan(loss) \"Loss is NaN, X_pred = $X_pred\"\n",
    "        end\n",
    "        loss /= length(state)\n",
    "        acc /= length(state)\n",
    "    end\n",
    "    loss /= length(data_loader)\n",
    "    acc /= length(data_loader)\n",
    "    # return loss, 100 * sqrt(loss)\n",
    "    return (loss = round(loss, digits = 4),\n",
    "            acc = round(100 * acc, digits = 2))\n",
    "end\n",
    "\n",
    "function train!(model; epochs=50, η=1e-2, infotime=10, log_loss=false)\n",
    "    # device = Flux.gpu # uncomment this for GPU training\n",
    "    device = Flux.cpu\n",
    "    model = model |> device\n",
    "    opt = ADAM()\n",
    "\n",
    "    function report(epoch)\n",
    "        train = eval_loss(model, train_loader, device)\n",
    "        test = eval_loss(model, test_loader, device)\n",
    "        @info (; epoch, train, test)\n",
    "    end\n",
    "\n",
    "    epoch_loss_vec = Float32[]\n",
    "    report(0)\n",
    "    for epoch in 1:epochs\n",
    "        epoch_loss = 0.0\n",
    "        for (g, state, species, y) in train_loader\n",
    "            g, state, y = MLUtils.batch(g) |> device, state |> device, y |> device\n",
    "\n",
    "            batch_loss = 0.0\n",
    "            loss_fn() = begin\n",
    "                X = model(g, g.ndata.x)\n",
    "                for (Xᵢ, stateᵢ, yᵢ) in zip(eachcol(X), state, y)\n",
    "                    V, T, Mw, m = stateᵢ\n",
    "                    ŷ = predict_a_res(Xᵢ, V, T, Mw, m)\n",
    "                    batch_loss += ((ŷ - yᵢ) / yᵢ)^2\n",
    "                    @assert batch_loss isa Real \"Loss is not a real number, got $(typeof(loss)), X_pred = $X_pred\"\n",
    "                    @assert !isnan(batch_loss) \"Loss is NaN, X_pred = $X_pred\"\n",
    "                end\n",
    "                batch_loss /= length(state)\n",
    "            end\n",
    "\n",
    "            grads = Zygote.gradient(Flux.params(model)) do\n",
    "                loss_fn()\n",
    "            end\n",
    "            epoch_loss += batch_loss\n",
    "            Flux.update!(opt, Flux.params(model), grads)\n",
    "        end\n",
    "        epoch_loss /= length(train_loader)\n",
    "        push!(epoch_loss_vec, epoch_loss)\n",
    "        \n",
    "        epoch % infotime == 0 && report(epoch)\n",
    "    end\n",
    "    return epoch_loss_vec\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: nh = 16\n",
      "└ @ Main /home/luc/SAFT_ML/4_gnn_saft.ipynb:3\n"
     ]
    },
    {
     "ename": "MethodError",
     "evalue": "MethodError: no method matching create_graphattention_model(::Int64, ::Int64; nhlayers=1)\nClosest candidates are:\n  create_graphattention_model(::Any, ::Any, !Matched::Any; nout, nhlayers, afunc) at ~/SAFT_ML/4_gnn_saft.ipynb:12",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching create_graphattention_model(::Int64, ::Int64; nhlayers=1)\n",
      "Closest candidates are:\n",
      "  create_graphattention_model(::Any, ::Any, !Matched::Any; nout, nhlayers, afunc) at ~/SAFT_ML/4_gnn_saft.ipynb:12\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/SAFT_ML/4_gnn_saft.ipynb:4"
     ]
    }
   ],
   "source": [
    "nin = 11\n",
    "for nh ∈ [16, 64, 128, 256]\n",
    "    @info \"nh = $nh\"\n",
    "    model = create_graphattention_model(nin, nh, nhlayers=1)\n",
    "    epoch_loss_vec = train!(model, epochs=50, infotime=50)\n",
    "end\n",
    "# nh = 16\n",
    "# model = create_graphattention_model(nin, nh, nhlayers=1)\n",
    "# model = create_graphconv_model(nin, nh; nhlayers=1)\n",
    "# epoch_loss_vec = train!(model, epochs=50, infotime=50)\n",
    "nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: nh = 16\n",
      "└ @ Main /home/luc/SAFT_ML/4_gnn_saft.ipynb:3\n",
      "┌ Info: (epoch = 0, train = (loss = 59.9701, acc = 122.26), test = (loss = 37.3289, acc = 136.72))\n",
      "└ @ Main /home/luc/SAFT_ML/4_gnn_saft.ipynb:73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: (epoch = 50, train = (loss = 0.5049, acc = 19.32), test = (loss = 2.1893, acc = 81.04))\n",
      "└ @ Main /home/luc/SAFT_ML/4_gnn_saft.ipynb:73\n",
      "┌ Info: nh = 64\n",
      "└ @ Main /home/luc/SAFT_ML/4_gnn_saft.ipynb:3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: (epoch = 0, train = (loss = 214.302, acc = 222.62), test = (loss = 85.6072, acc = 137.17))\n",
      "└ @ Main /home/luc/SAFT_ML/4_gnn_saft.ipynb:73\n",
      "┌ Info: (epoch = 50, train = (loss = 0.0689, acc = 7.9), test = (loss = 2.1431, acc = 80.62))\n",
      "└ @ Main /home/luc/SAFT_ML/4_gnn_saft.ipynb:73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: nh = 128\n",
      "└ @ Main /home/luc/SAFT_ML/4_gnn_saft.ipynb:3\n",
      "┌ Info: (epoch = 0, train = (loss = 16.8053, acc = 75.76), test = (loss = 246.3184, acc = 215.76))\n",
      "└ @ Main /home/luc/SAFT_ML/4_gnn_saft.ipynb:73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: (epoch = 50, train = (loss = 0.5947, acc = 22.98), test = (loss = 2.0602, acc = 78.56))\n",
      "└ @ Main /home/luc/SAFT_ML/4_gnn_saft.ipynb:73\n",
      "┌ Info: nh = 256\n",
      "└ @ Main /home/luc/SAFT_ML/4_gnn_saft.ipynb:3\n",
      "┌ Info: (epoch = 0, train = (loss = 165.2185, acc = 144.35), test = (loss = 16.0971, acc = 106.54))\n",
      "└ @ Main /home/luc/SAFT_ML/4_gnn_saft.ipynb:73\n",
      "┌ Info: (epoch = 50, train = (loss = 0.5371, acc = 19.26), test = (loss = 2.2351, acc = 81.73))\n",
      "└ @ Main /home/luc/SAFT_ML/4_gnn_saft.ipynb:73\n"
     ]
    }
   ],
   "source": [
    "nin = 11\n",
    "for nh ∈ [16, 64, 128, 256]\n",
    "    @info \"nh = $nh\"\n",
    "    model = create_graphconv_model(nin, nh; nhlayers=1)\n",
    "    epoch_loss_vec = train!(model, epochs=50, infotime=50)\n",
    "end\n",
    "nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Float64}:\n",
       "   3.626526253156598\n",
       "  13.501713312682039\n",
       " 217.52441395119735"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3-element Vector{Float64}:\n",
       "   4.2928\n",
       "  15.847\n",
       " 321.94"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(first(state), first(species), first(y)) = (Float32[0.028108373, 304.7953, 72.15, 1.9606], \"pentane\", -0.028997974f0)\n"
     ]
    }
   ],
   "source": [
    "# Abstract this into a function that takes a model and returns the X_bounded output\n",
    "g, state, species, y = first(train_loader)\n",
    "g = MLUtils.batch(g)\n",
    "X = model(g, g.ndata.x)\n",
    "X = first(eachcol(X))\n",
    "@show first(state), first(species), first(y)\n",
    "bounds = Tuple{Float32,Float32}[\n",
    "    (2.5, 5),\n",
    "    (10, 18),\n",
    "    (150, 300),\n",
    "]\n",
    "# ŷ = mean(X)\n",
    "X_bounded = bound_output(X, first.(bounds), last.(bounds))\n",
    "m = SAFTVRMie([first(species)])\n",
    "X_nominal = [m.params.sigma.values[1]*1e10, m.params.lambda_r.values[1], m.params.epsilon.values[1]]\n",
    "display(X_bounded)\n",
    "display(X_nominal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Float64}:\n",
       "   3.737e-10\n",
       "  12.504\n",
       " 152.58"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = SAFTVRMie([\"methane\"])\n",
    "X_nominal = [m.params.sigma.values[1], m.params.lambda_r.values[1], m.params.epsilon.values[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: epoch_loss_vec not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: epoch_loss_vec not defined\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/SAFT_ML/4_gnn_saft.ipynb:1"
     ]
    }
   ],
   "source": [
    "plot(epoch_loss_vec, label = \"Training loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: epoch_loss_vec not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: epoch_loss_vec not defined\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/SAFT_ML/4_gnn_saft.ipynb:1"
     ]
    }
   ],
   "source": [
    "plot(epoch_loss_vec, label = \"Training loss\", ylims=(0.0, 0.02))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next:\n",
    "# - Take notebooks, extract to .jl single-functions and run hyperparameter sweep\n",
    "# - Evaluate AAD on saturation pressure\n",
    "# - Stratify test/train data by molecule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
