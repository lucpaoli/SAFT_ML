{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/SAFT_ML`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefinition of constant UNIT_FORMATS. This may fail, cause incorrect answers, or produce other errors.\n"
     ]
    }
   ],
   "source": [
    "import Pkg; Pkg.activate(\".\")\n",
    "using Clapeyron\n",
    "includet(\"./saftvrmienn.jl\")\n",
    "# These are functions we're going to overload for SAFTVRMieNN\n",
    "import Clapeyron: a_res, saturation_pressure, pressure\n",
    "\n",
    "using Flux\n",
    "using Plots, Statistics\n",
    "using ForwardDiff, DiffResults\n",
    "\n",
    "using Zygote, ChainRulesCore\n",
    "using ImplicitDifferentiation\n",
    "\n",
    "using CSV, DataFrames\n",
    "using MLUtils\n",
    "using RDKitMinimalLib\n",
    "using JLD2\n",
    "\n",
    "# Multithreaded loss\n",
    "using Zygote: bufferfrom\n",
    "using Base.Threads: @spawn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([-0.0, 2.260005096581295e-5, 2.1863843008154135e-5, 1.46022174535494e-6, 1.6493904176981177e-7, -6.011351535625539e-8],)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = [16.04, 1.0, 3.737, 6.0, 12.504, 152.58]\n",
    "V = volume_NN(X, 1e7, 100.0)\n",
    "∂V∂X = Zygote.gradient(X -> volume_NN(X, 1e7, 100.0), X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×6 transpose(::Vector{Float64}) with eltype Float64:\n",
       " -0.0  -0.0  -0.0  -0.0  -0.0  -5.78614e-8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = [16.04, 1.0, 3.737, 6.0, 12.504, 152.58]\n",
    "f_V(X) = volume_NN(X, 1e7, 100.0)[1]\n",
    "dX = [0.0, 0.0, 0.0, 0.0, 0.0, 1e-6]\n",
    "f_∂V∂X(X) = (f_V(X .+ dX) - f_V(X .- dX))/(2dX)\n",
    "f_∂V∂X(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "main (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate training set for liquid density and saturation pressure\n",
    "function create_data(; batch_size=16, n_points=25)\n",
    "    # Create training & validation data\n",
    "    df = CSV.read(\"./pcpsaft_params/SI_pcp-saft_parameters.csv\", DataFrame, header=1)\n",
    "    filter!(row -> occursin(\"Alkane\", row.family), df)\n",
    "    df = first(df, 1) #* Take only first molecule in dataframe\n",
    "    @show df.common_name\n",
    "    mol_data = zip(df.common_name, df.isomeric_smiles, df.molarweight)\n",
    "    println(\"Generating data for $(length(mol_data)) molecules...\")\n",
    "\n",
    "    function make_fingerprint(s::String)::Vector{Float64}\n",
    "        mol = get_mol(s)\n",
    "        @assert !isnothing(mol)\n",
    "\n",
    "        fp = []\n",
    "        # for (nbits, rad) in [(256, 256), (1, 3)]\n",
    "        #* Approximately ECFP4 fingerprint\n",
    "        nbits = 256\n",
    "        rad = 4\n",
    "\n",
    "        fp_details = Dict{String,Any}(\"nBits\" => nbits, \"radius\" => rad)\n",
    "        fp_str = get_morgan_fp(mol, fp_details)\n",
    "        append!(fp, [parse(Float64, string(c)) for c in fp_str])\n",
    "        # end\n",
    "\n",
    "        desc = get_descriptors(mol)\n",
    "        relevant_keys = [\n",
    "            \"CrippenClogP\",\n",
    "            \"NumHeavyAtoms\",\n",
    "            \"amw\",\n",
    "            \"FractionCSP3\",\n",
    "        ]\n",
    "        relevant_desc = [desc[k] for k in relevant_keys]\n",
    "        append!(fp, last.(relevant_desc))\n",
    "\n",
    "        return fp\n",
    "    end\n",
    "\n",
    "    T = Float64\n",
    "    # X_data = Vector{Tuple{Vector{T},T,T,T}}([])\n",
    "    X_data = Vector{Tuple{Vector{T},T,T,T}}([])\n",
    "    Y_data = Vector{Vector{T}}()\n",
    "\n",
    "    # n = 0\n",
    "    for (name, smiles, Mw) in mol_data\n",
    "        # if n < 20\n",
    "        try\n",
    "            # saft_model = PPCSAFT([name])\n",
    "            saft_model = SAFTVRMie([name])\n",
    "            Tc, pc, Vc = crit_pure(saft_model)\n",
    "\n",
    "            # fp = make_fingerprint(smiles)\n",
    "            fp = [1.0]\n",
    "            # append!(fp, Mw)\n",
    "\n",
    "            T_range = range(0.5 * Tc, 0.975 * Tc, n_points)\n",
    "            for T in T_range\n",
    "                (p_sat, Vₗ_sat, Vᵥ_sat) = saturation_pressure(saft_model, T)\n",
    "\n",
    "                p = p_sat * 5.0\n",
    "\n",
    "                Vₗ = volume(saft_model, p, T; phase=:liquid)\n",
    "                push!(X_data, (fp, p, T, Mw))\n",
    "                push!(Y_data, [Vₗ, p_sat])\n",
    "            end\n",
    "            # n += 1\n",
    "        catch e\n",
    "            println(\"Fingerprint generation failed for $name, $e\")\n",
    "        end\n",
    "        # else\n",
    "        # break\n",
    "        # end\n",
    "    end\n",
    "\n",
    "    #* Remove columns from fingerprints\n",
    "    # Identify zero & one columns\n",
    "    # num_cols = length(X_data[1][1])\n",
    "    # zero_cols = trues(num_cols)\n",
    "    # for (vec, _, _) in X_data\n",
    "    #     zero_cols .&= (vec .== 0)\n",
    "    # end\n",
    "    # keep_cols = .!zero_cols # Create a Mask\n",
    "    # X_data = [(vec[keep_cols], vals...) for (vec, vals...) in X_data] # Apply Mask\n",
    "\n",
    "    # num_cols = length(X_data[1][1])\n",
    "    # one_cols = trues(num_cols)\n",
    "    # for (vec, _, _) in X_data\n",
    "    #     one_cols .&= (vec .== 1)\n",
    "    # end\n",
    "    # keep_cols = .!one_cols # Create a Mask\n",
    "    # X_data = [(vec[keep_cols], vals...) for (vec, vals...) in X_data] # Apply Mask\n",
    "\n",
    "    train_data, test_data = splitobs((X_data, Y_data), at=1.0, shuffle=false)\n",
    "\n",
    "    train_loader = DataLoader(train_data, batchsize=batch_size, shuffle=false)\n",
    "    test_loader = DataLoader(test_data, batchsize=batch_size, shuffle=false)\n",
    "    println(\"n_batches = $(length(train_loader)), batch_size = $batch_size\")\n",
    "    flush(stdout)\n",
    "    return train_loader, test_loader\n",
    "end\n",
    "\n",
    "\n",
    "function create_ff_model(nfeatures)\n",
    "    # Base NN architecture from \"Fitting Error vs Parameter Performance\"\n",
    "    nout = 3\n",
    "    model = Chain(\n",
    "        Dense(nfeatures, nout, x -> x; bias=false, init=zeros32),\n",
    "    )\n",
    "    # model = Chain(\n",
    "    #     Dense(nfeatures, nout * 8, relu),\n",
    "    #     Dense(nout * 8, nout * 4, relu),\n",
    "    #     Dense(nout * 4, nout * 2, relu),\n",
    "    #     Dense(nout * 2, nout, relu),\n",
    "    #     # Dense(16, nout, relu),\n",
    "    #     # Dense(16, nout, x -> x),\n",
    "    #     # Dense(1024, 512, relu),\n",
    "    #     # Dense(512, 256, relu),\n",
    "    #     # Dense(32, 32, relu),\n",
    "    #     # Dense(32, 32, relu),\n",
    "    #     # Dense(32, nout, relu),\n",
    "    # )\n",
    "    # model(x) = m, σ, λ_a, λ_r, ϵ\n",
    "\n",
    "    # return nn_model, unbounded_model\n",
    "    return model\n",
    "end\n",
    "\n",
    "function get_idx_from_iterator(iterator, idx)\n",
    "    data_iterator = iterate(iterator)\n",
    "    for _ in 1:idx-1\n",
    "        data_iterator = iterate(iterator, data_iterator[2])\n",
    "    end\n",
    "    return data_iterator[1]\n",
    "end\n",
    "\n",
    "\n",
    "# function SAFT_head(model, X; b=[3.0, 3.5, 7.0, 12.5, 250.0], c=10.0)\n",
    "# function SAFT_head(model, X; b=[2.0, 4.0], c=[1.0, 1.0])\n",
    "function SAFT_head(model, X; b=[2.5, 3.5, 250.0], c=Float64[1, 1, 100])\n",
    "    fp, p, T, Mw = X\n",
    "\n",
    "    # m = 1.8514\n",
    "    # σ = 4.0887\n",
    "    λ_a = 6.0\n",
    "    λ_r = 13.65\n",
    "    # ϵ = 273.64\n",
    "    # fp, p, T, Mw = X\n",
    "    pred_params = model(fp)\n",
    "\n",
    "    # Add bias and scale\n",
    "    biased_params = @. pred_params * c + b\n",
    "\n",
    "    saft_input = vcat(Mw, biased_params[1:2], [λ_a, λ_r], biased_params[3])\n",
    "\n",
    "    # saft_input = vcat(Mw, biased_params[1:2], [λ_a, λ_r], ϵ)\n",
    "    Vₗ = volume_NN(saft_input, p, T)\n",
    "\n",
    "    ŷ_1 = !isnan(Vₗ) ? Vₗ : 1e3\n",
    "\n",
    "    Tc = ignore_derivatives() do\n",
    "        critical_temperature_NN(saft_input)\n",
    "    end\n",
    "    # todo include saturation volumes in loss\n",
    "    if T < Tc\n",
    "        sat_p = saturation_pressure_NN(saft_input, T)\n",
    "        if !isnan(sat_p)\n",
    "            ŷ_2 = sat_p\n",
    "        else\n",
    "            # println(\"sat_p is NaN at T = $T, saft_input = $saft_input\")\n",
    "            ŷ_2 = nothing\n",
    "        end\n",
    "    else\n",
    "        ŷ_2 = nothing\n",
    "    end\n",
    "\n",
    "    return [ŷ_1, ŷ_2]\n",
    "end\n",
    "\n",
    "function eval_loss(X_batch, y_batch, metric, model)\n",
    "    batch_loss = 0.0\n",
    "    n = 0\n",
    "    for (X, y_vec) in zip(X_batch, y_batch)\n",
    "        # y = y_vec[1]\n",
    "        ŷ_vec = SAFT_head(model, X)\n",
    "\n",
    "        for (ŷ, y) in zip(ŷ_vec, y_vec)\n",
    "            if !isnothing(ŷ)\n",
    "                batch_loss += metric(y, ŷ)\n",
    "                n += 1\n",
    "            end\n",
    "        end\n",
    "\n",
    "    end\n",
    "    if n > 0\n",
    "        batch_loss /= n\n",
    "    end\n",
    "    # penalize batch_loss depending on how many failed\n",
    "    # batch_loss += length(y_batch) - n\n",
    "\n",
    "    return batch_loss\n",
    "end\n",
    "\n",
    "function eval_loss_par(X_batch, y_batch, metric, model, n_chunks)\n",
    "    n = length(X_batch)\n",
    "    chunk_size = n ÷ n_chunks\n",
    "\n",
    "    p = bufferfrom(zeros(n_chunks))\n",
    "\n",
    "    # Creating views for each chunk\n",
    "    X_chunks = vcat([view(X_batch, (i-1)*chunk_size+1:i*chunk_size) for i in 1:n_chunks-1], [view(X_batch, (n_chunks-1)*chunk_size+1:n)])\n",
    "    y_chunks = vcat([view(y_batch, (i-1)*chunk_size+1:i*chunk_size) for i in 1:n_chunks-1], [view(y_batch, (n_chunks-1)*chunk_size+1:n)])\n",
    "\n",
    "    @sync begin\n",
    "        for i = 1:n_chunks\n",
    "            @spawn begin\n",
    "                p[i] = eval_loss(X_chunks[i], y_chunks[i], metric, model)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return sum(p) / n_chunks # average partial losses\n",
    "end\n",
    "\n",
    "function percent_error(y, ŷ)\n",
    "    return 100 * abs(y - ŷ) / y\n",
    "end\n",
    "\n",
    "function mse(y, ŷ)\n",
    "    return ((y - ŷ) / y)^2\n",
    "end\n",
    "\n",
    "function finite_diff_grads(model, x, y; ϵ=1e-8)\n",
    "    grads = []\n",
    "    for p in Flux.params(model)\n",
    "        push!(grads, zeros(size(p)))\n",
    "    end\n",
    "\n",
    "    for (i, p) in enumerate(Flux.params(model))\n",
    "        for j in eachindex(p)\n",
    "            tmp = p[j]\n",
    "            p[j] = tmp + ϵ\n",
    "            J1 = eval_loss(x, y, mse, model)\n",
    "            p[j] = tmp - ϵ\n",
    "            J2 = eval_loss(x, y, mse, model)\n",
    "            p[j] = tmp\n",
    "            grads[i][j] = (J1 - J2) / (2 * ϵ)\n",
    "        end\n",
    "    end\n",
    "    return grads\n",
    "end\n",
    "\n",
    "function train_model!(model, train_loader, test_loader; epochs=10)\n",
    "    optim = Flux.setup(Flux.Adam(0.01), model) # 1e-3 usually safe starting LR\n",
    "    # optim = Flux.setup(Descent(0.001), model)\n",
    "\n",
    "    println(\"training on $(Threads.nthreads()) threads\")\n",
    "    flush(stdout)\n",
    "\n",
    "    for epoch in 1:epochs\n",
    "        batch_loss = 0.0\n",
    "        for (X_batch, y_batch) in train_loader\n",
    "\n",
    "            loss, grads = Flux.withgradient(model) do m\n",
    "                # loss = eval_loss_par(X_batch, y_batch, percent_error, m, Threads.nthreads())\n",
    "                loss = eval_loss(X_batch, y_batch, mse, m)\n",
    "                loss\n",
    "            end\n",
    "            batch_loss += loss\n",
    "            @assert !isnan(loss)\n",
    "\n",
    "            # grads_fd = finite_diff_grads(model, X_batch, y_batch)\n",
    "            # @show grads[1]\n",
    "            # @show grads_fd      # Show FD gradients\n",
    "\n",
    "            Flux.update!(optim, model, grads[1])\n",
    "        end\n",
    "        batch_loss /= length(train_loader)\n",
    "        epoch % 25 == 0 && println(\"epoch $epoch: batch_loss = $batch_loss\")\n",
    "        flush(stdout)\n",
    "    end\n",
    "end\n",
    "\n",
    "function main(; epochs=15)\n",
    "    train_loader, test_loader = create_data(n_points=21, batch_size=7) # Should make 5 batches / epoch. 256 / 8 gives 32 evaluations per thread\n",
    "    @show n_features = length(first(train_loader)[1][1][1])\n",
    "\n",
    "    model = create_ff_model(n_features)\n",
    "    @show model.layers[1].weight, model([1.0])\n",
    "    train_model!(model, train_loader, test_loader; epochs=epochs)\n",
    "    return model\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.common_name = [\"n-butane\"]\n",
      "Generating data for 1 molecules...\n",
      "n_batches = 3, batch_size = 7"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_features = length((((first(train_loader))[1])[1])[1]) = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Layer with Float32 parameters got Float64 input.\n",
      "│   The input will be converted, but any earlier layers may be very slow.\n",
      "│   layer = Dense(1 => 3, #93; bias=false)\n",
      "│   summary(x) = 1-element Vector{Float64}\n",
      "└ @ Flux /home/luc/.julia/packages/Flux/n3cOc/src/layers/stateless.jl:60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((model.layers[1]).weight, model([1.0])) = (Float32[0.0; 0.0; 0.0;;], Float32[0.0, 0.0, 0.0])\n",
      "training on 1 threads"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 25: batch_loss = 0.005619915107475788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50: batch_loss = 0.00615880051647071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 75: batch_loss = 0.005483330205603552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100: batch_loss = 0.004858577804311752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 125: batch_loss = 0.004284375236292748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 150: batch_loss = 0.003760996525776883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 175: batch_loss = 0.003287093872920531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 200: batch_loss = 0.0029119732615235615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 225: batch_loss = 0.0025081942696167305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 250: batch_loss = 0.002150861877225279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 275: batch_loss = 0.0018351611139364957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 300: batch_loss = 0.0015575038724551197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 325: batch_loss = 0.0013144607002939322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 350: batch_loss = 0.0011027887981631436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 375: batch_loss = 0.0009194350781812667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 400: batch_loss = 0.000761517773962095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 425: batch_loss = 0.0006263361316016846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 450: batch_loss = 0.0005113685592474266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 475: batch_loss = 0.00041426498516970955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 500: batch_loss = 0.00033285368185187637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 525: batch_loss = 0.0002651308066611661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 550: batch_loss = 0.00020926000670961272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 575: batch_loss = 0.00016357577883438168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 600: batch_loss = 0.00012656970752279219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 625: batch_loss = 9.68891819913525e-5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 650: batch_loss = 7.333560686269329e-5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 675: batch_loss = 5.4851540289041846e-5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 700: batch_loss = 4.051614576591616e-5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 725: batch_loss = 2.953623023281745e-5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 750: batch_loss = 2.1236287648621446e-5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 775: batch_loss = 1.504816178385457e-5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 800: batch_loss = 1.050209991963872e-5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 825: batch_loss = 7.2130497829589605e-6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 850: batch_loss = 4.871791511985847e-6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 875: batch_loss = 3.2332156392525673e-6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 900: batch_loss = 2.1065505110066094e-6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 925: batch_loss = 1.3461283402344606e-6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 950: batch_loss = 8.430577425253374e-7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 975: batch_loss = 5.168488114194139e-7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1000: batch_loss = 3.0985180570194125e-7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Dense(1 => 3, #93; bias=false),       \u001b[90m# 3 parameters\u001b[39m\n",
       ") "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = main(;epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×1 Matrix{Float32}:\n",
       " -0.6451405\n",
       "  0.5863259\n",
       "  0.23365809"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fieldnames(typeof(m.layers[1]))\n",
    "m.layers[1].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Float64}:\n",
       "   1.854859471321106\n",
       "   4.0863258838653564\n",
       " 273.3658090233803"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "b = [2.5, 3.5, 250.0]\n",
    "# b = [2.0, 4.0, 250.0]\n",
    "c = Float64[1, 1, 100]\n",
    "# b = [2.0, 4.0]\n",
    "# c = Float64[1, 1]\n",
    "m([1.0]) .* c .+ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.8514, 4.0887, 273.64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = CSV.read(\"./pcpsaft_params/SI_pcp-saft_parameters.csv\", DataFrame, header=1)\n",
    "filter!(row -> occursin(\"Alkane\", row.family), df)\n",
    "df = first(df, 1) #* Take only first molecule in dataframe\n",
    "mol_data = zip(df.common_name, df.isomeric_smiles, df.molarweight)\n",
    "saft_model = SAFTVRMie([first(mol_data)[1]])\n",
    "saft_model.params.segment.values[1], saft_model.params.sigma.values[1]*1e10, saft_model.params.epsilon.values[1]#, saft_model.params.lambda_a.values, saft_model.params.lambda_r.values, saft_model.params.epsilon.values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
