{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/SAFT_ML`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefinition of constant UNIT_FORMATS. This may fail, cause incorrect answers, or produce other errors.\n"
     ]
    }
   ],
   "source": [
    "import Pkg; Pkg.activate(\".\")\n",
    "using Clapeyron\n",
    "includet(\"./saftvrmienn.jl\")\n",
    "# These are functions we're going to overload for SAFTVRMieNN\n",
    "import Clapeyron: a_res, saturation_pressure, pressure\n",
    "\n",
    "using Flux\n",
    "using Plots, Statistics\n",
    "using ForwardDiff, DiffResults\n",
    "\n",
    "using Zygote, ChainRulesCore\n",
    "using ImplicitDifferentiation\n",
    "\n",
    "using CSV, DataFrames\n",
    "using MLUtils\n",
    "using RDKitMinimalLib\n",
    "using JLD2\n",
    "\n",
    "# Multithreaded loss\n",
    "using Zygote: bufferfrom\n",
    "using Base.Threads: @spawn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([-0.0, 2.260005096581295e-5, 2.1863843008154135e-5, 1.46022174535494e-6, 1.6493904176981177e-7, -6.011351535625539e-8],)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = [16.04, 1.0, 3.737, 6.0, 12.504, 152.58]\n",
    "V = volume_NN(X, 1e7, 100.0)\n",
    "∂V∂X = Zygote.gradient(X -> volume_NN(X, 1e7, 100.0), X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×6 transpose(::Vector{Float64}) with eltype Float64:\n",
       " -0.0  -0.0  -0.0  -0.0  -0.0  -5.78614e-8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = [16.04, 1.0, 3.737, 6.0, 12.504, 152.58]\n",
    "f_V(X) = volume_NN(X, 1e7, 100.0)[1]\n",
    "dX = [0.0, 0.0, 0.0, 0.0, 0.0, 1e-6]\n",
    "f_∂V∂X(X) = (f_V(X .+ dX) - f_V(X .- dX))/(2dX)\n",
    "f_∂V∂X(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "main (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate training set for liquid density and saturation pressure\n",
    "function create_data(; batch_size=16, n_points=25)\n",
    "    # Create training & validation data\n",
    "    df = CSV.read(\"./pcpsaft_params/SI_pcp-saft_parameters.csv\", DataFrame, header=1)\n",
    "    filter!(row -> occursin(\"Alkane\", row.family), df)\n",
    "    df = first(df, 1) #* Take only first molecule in dataframe\n",
    "    mol_data = zip(df.common_name, df.isomeric_smiles, df.molarweight)\n",
    "    println(\"Generating data for $(length(mol_data)) molecules...\")\n",
    "\n",
    "    function make_fingerprint(s::String)::Vector{Float64}\n",
    "        mol = get_mol(s)\n",
    "        @assert !isnothing(mol)\n",
    "\n",
    "        fp = []\n",
    "        # for (nbits, rad) in [(256, 256), (1, 3)]\n",
    "        #* Approximately ECFP4 fingerprint\n",
    "        nbits = 256\n",
    "        rad = 4\n",
    "\n",
    "        fp_details = Dict{String,Any}(\"nBits\" => nbits, \"radius\" => rad)\n",
    "        fp_str = get_morgan_fp(mol, fp_details)\n",
    "        append!(fp, [parse(Float64, string(c)) for c in fp_str])\n",
    "        # end\n",
    "\n",
    "        desc = get_descriptors(mol)\n",
    "        relevant_keys = [\n",
    "            \"CrippenClogP\",\n",
    "            \"NumHeavyAtoms\",\n",
    "            \"amw\",\n",
    "            \"FractionCSP3\",\n",
    "        ]\n",
    "        relevant_desc = [desc[k] for k in relevant_keys]\n",
    "        append!(fp, last.(relevant_desc))\n",
    "\n",
    "        return fp\n",
    "    end\n",
    "\n",
    "    T = Float64\n",
    "    X_data = Vector{Tuple{Vector{T},T,T,T}}([])\n",
    "    Y_data = Vector{Vector{T}}()\n",
    "\n",
    "    for (name, smiles, Mw) in mol_data\n",
    "        try\n",
    "            # saft_model = PPCSAFT([name])\n",
    "            saft_model = SAFTVRMie([name])\n",
    "            Tc, pc, Vc = crit_pure(saft_model)\n",
    "\n",
    "            # fp = make_fingerprint(smiles)\n",
    "            fp = [1.0]\n",
    "            # append!(fp, Mw)\n",
    "\n",
    "            T_range = range(0.5 * Tc, 0.975 * Tc, n_points)\n",
    "            for T in T_range\n",
    "                (p₀, V_vec...) = saturation_pressure(saft_model, T)\n",
    "                p = p₀ * 1.5\n",
    "                Vₗ = volume(saft_model, p, T; phase=:liquid)\n",
    "                push!(X_data, (fp, p, T, Mw))\n",
    "                push!(Y_data, [Vₗ])\n",
    "            end\n",
    "        catch e\n",
    "            println(\"Fingerprint generation failed for $name, $e\")\n",
    "        end\n",
    "    end\n",
    "\n",
    "    #* Remove columns from fingerprints\n",
    "    # Identify zero & one columns\n",
    "    # num_cols = length(X_data[1][1])\n",
    "    # zero_cols = trues(num_cols)\n",
    "    # for (vec, _, _) in X_data\n",
    "    #     zero_cols .&= (vec .== 0)\n",
    "    # end\n",
    "    # keep_cols = .!zero_cols # Create a Mask\n",
    "    # X_data = [(vec[keep_cols], vals...) for (vec, vals...) in X_data] # Apply Mask\n",
    "\n",
    "    # num_cols = length(X_data[1][1])\n",
    "    # one_cols = trues(num_cols)\n",
    "    # for (vec, _, _) in X_data\n",
    "    #     one_cols .&= (vec .== 1)\n",
    "    # end\n",
    "    # keep_cols = .!one_cols # Create a Mask\n",
    "    # X_data = [(vec[keep_cols], vals...) for (vec, vals...) in X_data] # Apply Mask\n",
    "\n",
    "    train_data, test_data = splitobs((X_data, Y_data), at=0.8, shuffle=false)\n",
    "\n",
    "    train_loader = DataLoader(train_data, batchsize=batch_size, shuffle=false)\n",
    "    test_loader = DataLoader(test_data, batchsize=batch_size, shuffle=false)\n",
    "    println(\"n_batches = $(length(train_loader)), batch_size = $batch_size\")\n",
    "    flush(stdout)\n",
    "    return train_loader, test_loader\n",
    "end\n",
    "\n",
    "\n",
    "function create_ff_model(nfeatures)\n",
    "    # Base NN architecture from \"Fitting Error vs Parameter Performance\"\n",
    "    nout = 5\n",
    "    model = Chain(\n",
    "        Dense(nfeatures, nout*8, relu),\n",
    "        Dense(nout*8, nout*4, relu),\n",
    "        Dense(nout*4, nout*2, relu),\n",
    "        Dense(nout*2, nout, relu),\n",
    "        # Dense(16, nout, relu),\n",
    "        # Dense(16, nout, x -> x),\n",
    "        # Dense(1024, 512, relu),\n",
    "        # Dense(512, 256, relu),\n",
    "        # Dense(32, 32, relu),\n",
    "        # Dense(32, 32, relu),\n",
    "        # Dense(32, nout, relu),\n",
    "    )\n",
    "    # model(x) = m, σ, λ_a, λ_r, ϵ\n",
    "\n",
    "    # return nn_model, unbounded_model\n",
    "    return model\n",
    "end\n",
    "\n",
    "function get_idx_from_iterator(iterator, idx)\n",
    "    data_iterator = iterate(iterator)\n",
    "    for _ in 1:idx-1\n",
    "        data_iterator = iterate(iterator, data_iterator[2])\n",
    "    end\n",
    "    return data_iterator[1]\n",
    "end\n",
    "\n",
    "\n",
    "function SAFT_head(model, X; b=[3.0, 3.5, 7.0, 12.5, 250.0], c=10.0)\n",
    "    fp, p, T, Mw = X\n",
    "    pred_params = model(fp)\n",
    "    # Add bias and scale\n",
    "    biased_params = pred_params / c + b\n",
    "\n",
    "    saft_input = vcat(Mw, biased_params)\n",
    "    Vₗ = volume_NN(saft_input, p, T)\n",
    "    ŷ = Vₗ\n",
    "    # ŷ = sum(saft_input)\n",
    "    # Tc = ignore_derivatives() do\n",
    "    #     critical_temperature_NN(saft_input)\n",
    "    # end\n",
    "    # if T < Tc\n",
    "    #     sat_p = saturation_pressure_NN(saft_input, T)\n",
    "    #     if !isnan(sat_p)\n",
    "    #         ŷ = sat_p\n",
    "    #     else\n",
    "    #         # println(\"sat_p is NaN at T = $T, saft_input = $saft_input\")\n",
    "    #         ŷ = nothing\n",
    "    #     end\n",
    "    # else\n",
    "    #     ŷ = nothing\n",
    "    # end\n",
    "\n",
    "    return ŷ\n",
    "end\n",
    "\n",
    "function eval_loss(X_batch, y_batch, metric, model)\n",
    "    batch_loss = 0.0\n",
    "    n = 0\n",
    "    for (X, y_vec) in zip(X_batch, y_batch)\n",
    "        y = y_vec[1]\n",
    "        ŷ = SAFT_head(model, X)\n",
    "        if !isnothing(ŷ)\n",
    "            batch_loss += metric(y, ŷ)\n",
    "            n += 1\n",
    "        end\n",
    "    end\n",
    "    if n > 0 \n",
    "        batch_loss /= n\n",
    "    end\n",
    "    # penalize batch_loss depending on how many failed\n",
    "    batch_loss += length(y_batch) - n\n",
    "\n",
    "    return batch_loss\n",
    "end\n",
    "\n",
    "function eval_loss_par(X_batch, y_batch, metric, model, n_chunks)\n",
    "    n = length(X_batch)\n",
    "    chunk_size = n ÷ n_chunks\n",
    "\n",
    "    p = bufferfrom(zeros(n_chunks))\n",
    "\n",
    "    # Creating views for each chunk\n",
    "    X_chunks = vcat([view(X_batch, (i-1)*chunk_size+1:i*chunk_size) for i in 1:n_chunks-1], [view(X_batch, (n_chunks-1)*chunk_size+1:n)])\n",
    "    y_chunks = vcat([view(y_batch, (i-1)*chunk_size+1:i*chunk_size) for i in 1:n_chunks-1], [view(y_batch, (n_chunks-1)*chunk_size+1:n)])\n",
    "\n",
    "    @sync begin\n",
    "        for i = 1:n_chunks\n",
    "            @spawn begin \n",
    "                p[i] = eval_loss(X_chunks[i], y_chunks[i], metric, model)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return sum(p) / n_chunks # average partial losses\n",
    "end\n",
    "\n",
    "function percent_error(y, ŷ)\n",
    "    return 100 * abs(y - ŷ) / y\n",
    "end\n",
    "\n",
    "function mse(y, ŷ)\n",
    "    return (y - ŷ)^2\n",
    "end\n",
    "\n",
    "function train_model!(model, train_loader, test_loader; epochs=10)\n",
    "    optim = Flux.setup(Flux.Adam(0.001), model) # 1e-3 usually safe starting LR\n",
    "\n",
    "    println(\"training on $(Threads.nthreads()) threads\")\n",
    "    flush(stdout)\n",
    "\n",
    "    for epoch in 1:epochs\n",
    "        batch_loss = 0.0\n",
    "        for (X_batch, y_batch) in train_loader\n",
    "\n",
    "            loss, grads = Flux.withgradient(model) do m\n",
    "                # loss = eval_loss_par(X_batch, y_batch, percent_error, m, Threads.nthreads())\n",
    "                loss = eval_loss(X_batch, y_batch, percent_error, m)\n",
    "                loss\n",
    "            end\n",
    "            batch_loss += loss\n",
    "            @assert !isnan(loss)\n",
    "\n",
    "            Flux.update!(optim, model, grads[1])\n",
    "        end\n",
    "        batch_loss /= length(train_loader)\n",
    "        epoch % 25 == 0 && println(\"epoch $epoch: batch_loss = $batch_loss\")\n",
    "        flush(stdout)\n",
    "    end\n",
    "end\n",
    "\n",
    "function main(;epochs=15)\n",
    "    train_loader, test_loader = create_data(n_points=40, batch_size=32) # Should make 5 batches / epoch. 256 / 8 gives 32 evaluations per thread\n",
    "    n_features = length(first(train_loader)[1][1][1])\n",
    "\n",
    "    model = create_ff_model(n_features)\n",
    "    train_model!(model, train_loader, test_loader; epochs=epochs)\n",
    "    return model\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data for 1 molecules...\n",
      "n_batches = 1, batch_size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on 1 threads\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 25: batch_loss = 9.242274525578019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50: batch_loss = 9.218041463446953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 75: batch_loss = 9.1754327757208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100: batch_loss = 9.09752602689555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 125: batch_loss = 8.9506153816173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 150: batch_loss = 8.779983405275496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 175: batch_loss = 8.522244616063816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 200: batch_loss = 8.147972954979284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 225: batch_loss = 7.688389637370618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 250: batch_loss = 7.065613981395191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 275: batch_loss = 6.265253465256205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 300: batch_loss = 5.282424080702349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 325: batch_loss = 4.208476258923702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 350: batch_loss = 2.9770374781779974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 375: batch_loss = 1.6247487279772053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 400: batch_loss = 0.6792526953527332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 425: batch_loss = 0.6199522504326581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 450: batch_loss = 0.6195588667338581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 475: batch_loss = 0.6195577069538356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 500: batch_loss = 0.6196034368776722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 525: batch_loss = 0.6195600547982286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 550: batch_loss = 0.6195750107928558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 575: batch_loss = 0.6195887851861731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 600: batch_loss = 0.6195602653554133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 625: batch_loss = 0.6195599676672674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 650: batch_loss = 0.619559439366473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 675: batch_loss = 0.6195596397317972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 700: batch_loss = 0.6195599221086124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 725: batch_loss = 0.619559918196734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 750: batch_loss = 0.6195604061569904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 775: batch_loss = 0.619573531612702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 800: batch_loss = 0.6195603085423035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 825: batch_loss = 0.6195600496306792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 850: batch_loss = 0.6195602140529343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 875: batch_loss = 0.6195601420358637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 900: batch_loss = 0.6195800491765063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 925: batch_loss = 0.6195737627352352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 950: batch_loss = 0.619560358227653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 975: batch_loss = 0.6195844865576995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1000: batch_loss = 0.6195983065842929\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Dense(1 => 40, relu),                 \u001b[90m# 80 parameters\u001b[39m\n",
       "  Dense(40 => 20, relu),                \u001b[90m# 820 parameters\u001b[39m\n",
       "  Dense(20 => 10, relu),                \u001b[90m# 210 parameters\u001b[39m\n",
       "  Dense(10 => 5, relu),                 \u001b[90m# 55 parameters\u001b[39m\n",
       ") \u001b[90m                  # Total: 8 arrays, \u001b[39m1_165 parameters, 5.051 KiB."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = main(;epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data for 1 molecules...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_batches = 1, batch_size = 32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5-element Vector{Float64}:\n",
       "   3.0\n",
       "   3.5\n",
       "   7.0\n",
       "  12.5\n",
       " 280.8042358398437"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loader, test_loader = create_data(n_points=40, batch_size=32) # Should make 5 batches / epoch. 256 / 8 gives 32 evaluations per thread\n",
    "mol = first(train_loader)[1][1]\n",
    "fp, p, T, Mw = mol\n",
    "\n",
    "b = [3.0, 3.5, 7.0, 12.5, 250.0]\n",
    "c = 10.0\n",
    "pred_params = m(fp)\n",
    "biased_params = pred_params / c + b\n",
    "\n",
    "biased_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1.8514], [4.0887000000000006e-10;;], [6.0;;], [13.65;;], [273.64;;])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = CSV.read(\"./pcpsaft_params/SI_pcp-saft_parameters.csv\", DataFrame, header=1)\n",
    "filter!(row -> occursin(\"Alkane\", row.family), df)\n",
    "df = first(df, 1) #* Take only first molecule in dataframe\n",
    "mol_data = zip(df.common_name, df.isomeric_smiles, df.molarweight)\n",
    "saft_model = SAFTVRMie([first(mol_data)[1]])\n",
    "saft_model.params.segment.values, saft_model.params.sigma.values, saft_model.params.lambda_a.values, saft_model.params.lambda_r.values, saft_model.params.epsilon.values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
