{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/SAFT_ML`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefinition of constant UNIT_FORMATS. This may fail, cause incorrect answers, or produce other errors.\n"
     ]
    }
   ],
   "source": [
    "import Pkg; Pkg.activate(\".\")\n",
    "using Clapeyron\n",
    "includet(\"./saftvrmienn.jl\")\n",
    "# These are functions we're going to overload for SAFTVRMieNN\n",
    "import Clapeyron: a_res, saturation_pressure, pressure\n",
    "\n",
    "using Flux\n",
    "using Plots, Statistics\n",
    "using ForwardDiff, DiffResults\n",
    "\n",
    "using Zygote, ChainRulesCore\n",
    "using ImplicitDifferentiation\n",
    "\n",
    "using CSV, DataFrames\n",
    "using MLUtils\n",
    "using RDKitMinimalLib\n",
    "using JLD2\n",
    "\n",
    "# Multithreaded loss\n",
    "using Zygote: bufferfrom\n",
    "using Base.Threads: @spawn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([-0.0, 2.260005096581295e-5, 2.1863843008154135e-5, 1.46022174535494e-6, 1.6493904176981177e-7, -6.011351535625539e-8],)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = [16.04, 1.0, 3.737, 6.0, 12.504, 152.58]\n",
    "V = volume_NN(X, 1e7, 100.0)\n",
    "∂V∂X = Zygote.gradient(X -> volume_NN(X, 1e7, 100.0), X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×6 transpose(::Vector{Float64}) with eltype Float64:\n",
       " -0.0  -0.0  -0.0  -0.0  -0.0  -5.78614e-8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = [16.04, 1.0, 3.737, 6.0, 12.504, 152.58]\n",
    "f_V(X) = volume_NN(X, 1e7, 100.0)[1]\n",
    "dX = [0.0, 0.0, 0.0, 0.0, 0.0, 1e-6]\n",
    "f_∂V∂X(X) = (f_V(X .+ dX) - f_V(X .- dX))/(2dX)\n",
    "f_∂V∂X(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "main (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate training set for liquid density and saturation pressure\n",
    "function create_data(; batch_size=16, n_points=25)\n",
    "    # Create training & validation data\n",
    "    df = CSV.read(\"./pcpsaft_params/SI_pcp-saft_parameters.csv\", DataFrame, header=1)\n",
    "    filter!(row -> occursin(\"Alkane\", row.family), df)\n",
    "    df = first(df, 1) #* Take only first molecule in dataframe\n",
    "    @show df.common_name\n",
    "    mol_data = zip(df.common_name, df.isomeric_smiles, df.molarweight)\n",
    "    println(\"Generating data for $(length(mol_data)) molecules...\")\n",
    "\n",
    "    function make_fingerprint(s::String)::Vector{Float64}\n",
    "        mol = get_mol(s)\n",
    "        @assert !isnothing(mol)\n",
    "\n",
    "        fp = []\n",
    "        # for (nbits, rad) in [(256, 256), (1, 3)]\n",
    "        #* Approximately ECFP4 fingerprint\n",
    "        nbits = 256\n",
    "        rad = 4\n",
    "\n",
    "        fp_details = Dict{String,Any}(\"nBits\" => nbits, \"radius\" => rad)\n",
    "        fp_str = get_morgan_fp(mol, fp_details)\n",
    "        append!(fp, [parse(Float64, string(c)) for c in fp_str])\n",
    "        # end\n",
    "\n",
    "        desc = get_descriptors(mol)\n",
    "        relevant_keys = [\n",
    "            \"CrippenClogP\",\n",
    "            \"NumHeavyAtoms\",\n",
    "            \"amw\",\n",
    "            \"FractionCSP3\",\n",
    "        ]\n",
    "        relevant_desc = [desc[k] for k in relevant_keys]\n",
    "        append!(fp, last.(relevant_desc))\n",
    "\n",
    "        return fp\n",
    "    end\n",
    "\n",
    "    T = Float64\n",
    "    X_data = Vector{Tuple{Vector{T},T,T,T}}([])\n",
    "    Y_data = Vector{Vector{T}}()\n",
    "\n",
    "    # n = 0\n",
    "    for (name, smiles, Mw) in mol_data\n",
    "        # if n < 20\n",
    "        try\n",
    "            # saft_model = PPCSAFT([name])\n",
    "            saft_model = SAFTVRMie([name])\n",
    "            Tc, pc, Vc = crit_pure(saft_model)\n",
    "\n",
    "            # fp = make_fingerprint(smiles)\n",
    "            fp = [1.0]\n",
    "            # append!(fp, Mw)\n",
    "\n",
    "            T_range = range(0.5 * Tc, 0.975 * Tc, n_points)\n",
    "            for T in T_range\n",
    "                (p₀, V_vec...) = saturation_pressure(saft_model, T)\n",
    "\n",
    "                p = p₀ * 10\n",
    "\n",
    "                Vₗ = volume(saft_model, p, T; phase=:liquid)\n",
    "                push!(X_data, (fp, p, T, Mw))\n",
    "                push!(Y_data, [Vₗ])\n",
    "            end\n",
    "            # n += 1\n",
    "        catch e\n",
    "            println(\"Fingerprint generation failed for $name, $e\")\n",
    "        end\n",
    "        # else\n",
    "        # break\n",
    "        # end\n",
    "    end\n",
    "\n",
    "    #* Remove columns from fingerprints\n",
    "    # Identify zero & one columns\n",
    "    # num_cols = length(X_data[1][1])\n",
    "    # zero_cols = trues(num_cols)\n",
    "    # for (vec, _, _) in X_data\n",
    "    #     zero_cols .&= (vec .== 0)\n",
    "    # end\n",
    "    # keep_cols = .!zero_cols # Create a Mask\n",
    "    # X_data = [(vec[keep_cols], vals...) for (vec, vals...) in X_data] # Apply Mask\n",
    "\n",
    "    # num_cols = length(X_data[1][1])\n",
    "    # one_cols = trues(num_cols)\n",
    "    # for (vec, _, _) in X_data\n",
    "    #     one_cols .&= (vec .== 1)\n",
    "    # end\n",
    "    # keep_cols = .!one_cols # Create a Mask\n",
    "    # X_data = [(vec[keep_cols], vals...) for (vec, vals...) in X_data] # Apply Mask\n",
    "\n",
    "    train_data, test_data = splitobs((X_data, Y_data), at=1.0, shuffle=false)\n",
    "\n",
    "    train_loader = DataLoader(train_data, batchsize=batch_size, shuffle=false)\n",
    "    test_loader = DataLoader(test_data, batchsize=batch_size, shuffle=false)\n",
    "    println(\"n_batches = $(length(train_loader)), batch_size = $batch_size\")\n",
    "    flush(stdout)\n",
    "    return train_loader, test_loader\n",
    "end\n",
    "\n",
    "\n",
    "function create_ff_model(nfeatures)\n",
    "    # Base NN architecture from \"Fitting Error vs Parameter Performance\"\n",
    "    nout = 3\n",
    "    model = Chain(\n",
    "        Dense(nfeatures, nout, x -> x; bias=false, init=zeros32),\n",
    "    )\n",
    "    # model = Chain(\n",
    "    #     Dense(nfeatures, nout * 8, relu),\n",
    "    #     Dense(nout * 8, nout * 4, relu),\n",
    "    #     Dense(nout * 4, nout * 2, relu),\n",
    "    #     Dense(nout * 2, nout, relu),\n",
    "    #     # Dense(16, nout, relu),\n",
    "    #     # Dense(16, nout, x -> x),\n",
    "    #     # Dense(1024, 512, relu),\n",
    "    #     # Dense(512, 256, relu),\n",
    "    #     # Dense(32, 32, relu),\n",
    "    #     # Dense(32, 32, relu),\n",
    "    #     # Dense(32, nout, relu),\n",
    "    # )\n",
    "    # model(x) = m, σ, λ_a, λ_r, ϵ\n",
    "\n",
    "    # return nn_model, unbounded_model\n",
    "    return model\n",
    "end\n",
    "\n",
    "function get_idx_from_iterator(iterator, idx)\n",
    "    data_iterator = iterate(iterator)\n",
    "    for _ in 1:idx-1\n",
    "        data_iterator = iterate(iterator, data_iterator[2])\n",
    "    end\n",
    "    return data_iterator[1]\n",
    "end\n",
    "\n",
    "\n",
    "# function SAFT_head(model, X; b=[3.0, 3.5, 7.0, 12.5, 250.0], c=10.0)\n",
    "# function SAFT_head(model, X; b=[3.0, 3.5], c=10.0)\n",
    "function SAFT_head(model, X; b=[2.0, 4.0, 250.0], c=Float64[1, 1, 100])\n",
    "\n",
    "    # m = 1.8514\n",
    "    # σ = 4.0887\n",
    "    λ_a = 6.0\n",
    "    λ_r = 13.65\n",
    "    # ϵ = 273.64\n",
    "    fp, p, T, Mw = X\n",
    "    pred_params = model(fp)\n",
    "\n",
    "    # Add bias and scale\n",
    "    biased_params = @. pred_params * c + b\n",
    "\n",
    "    saft_input = vcat(Mw, biased_params[1:2], [λ_a, λ_r], biased_params[3])\n",
    "    # @show saft_input, p, T\n",
    "    Vₗ = volume_NN(saft_input, p, T)\n",
    "\n",
    "    ŷ = !isnan(Vₗ) ? Vₗ : 1e3\n",
    "    # if !isnan(Vₗ)\n",
    "    # ŷ = Vₗ\n",
    "    # else\n",
    "\n",
    "    # ŷ = sum(saft_input)\n",
    "    # Tc = ignore_derivatives() do\n",
    "    #     critical_temperature_NN(saft_input)\n",
    "    # end\n",
    "    # if T < Tc\n",
    "    #     sat_p = saturation_pressure_NN(saft_input, T)\n",
    "    #     if !isnan(sat_p)\n",
    "    #         ŷ = sat_p\n",
    "    #     else\n",
    "    #         # println(\"sat_p is NaN at T = $T, saft_input = $saft_input\")\n",
    "    #         ŷ = nothing\n",
    "    #     end\n",
    "    # else\n",
    "    #     ŷ = nothing\n",
    "    # end\n",
    "\n",
    "    return ŷ\n",
    "end\n",
    "\n",
    "function eval_loss(X_batch, y_batch, metric, model)\n",
    "    batch_loss = 0.0\n",
    "    n = 0\n",
    "    for (X, y_vec) in zip(X_batch, y_batch)\n",
    "        y = y_vec[1]\n",
    "        ŷ = SAFT_head(model, X)\n",
    "        if !isnothing(ŷ)\n",
    "            batch_loss += metric(y, ŷ)\n",
    "            n += 1\n",
    "        end\n",
    "    end\n",
    "    if n > 0\n",
    "        batch_loss /= n\n",
    "    end\n",
    "    # penalize batch_loss depending on how many failed\n",
    "    # batch_loss += length(y_batch) - n\n",
    "\n",
    "    return batch_loss\n",
    "end\n",
    "\n",
    "function eval_loss_par(X_batch, y_batch, metric, model, n_chunks)\n",
    "    n = length(X_batch)\n",
    "    chunk_size = n ÷ n_chunks\n",
    "\n",
    "    p = bufferfrom(zeros(n_chunks))\n",
    "\n",
    "    # Creating views for each chunk\n",
    "    X_chunks = vcat([view(X_batch, (i-1)*chunk_size+1:i*chunk_size) for i in 1:n_chunks-1], [view(X_batch, (n_chunks-1)*chunk_size+1:n)])\n",
    "    y_chunks = vcat([view(y_batch, (i-1)*chunk_size+1:i*chunk_size) for i in 1:n_chunks-1], [view(y_batch, (n_chunks-1)*chunk_size+1:n)])\n",
    "\n",
    "    @sync begin\n",
    "        for i = 1:n_chunks\n",
    "            @spawn begin\n",
    "                p[i] = eval_loss(X_chunks[i], y_chunks[i], metric, model)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return sum(p) / n_chunks # average partial losses\n",
    "end\n",
    "\n",
    "function percent_error(y, ŷ)\n",
    "    return 100 * abs(y - ŷ) / y\n",
    "end\n",
    "\n",
    "function mse(y, ŷ)\n",
    "    return (y - ŷ)^2 / y\n",
    "end\n",
    "\n",
    "function finite_diff_grads(model, x, y; ϵ=1e-5)\n",
    "    grads = []\n",
    "    for p in Flux.params(model)\n",
    "        push!(grads, zeros(size(p)))\n",
    "    end\n",
    "\n",
    "    for (i, p) in enumerate(Flux.params(model))\n",
    "        for j in eachindex(p)\n",
    "            tmp = p[j]\n",
    "            p[j] = tmp + ϵ\n",
    "            J1 = eval_loss(x, y, percent_error, model)\n",
    "            p[j] = tmp - ϵ\n",
    "            J2 = eval_loss(x, y, percent_error, model)\n",
    "            p[j] = tmp\n",
    "            grads[i][j] = (J1 - J2) / (2 * ϵ)\n",
    "        end\n",
    "    end\n",
    "    return grads\n",
    "end\n",
    "\n",
    "function train_model!(model, train_loader, test_loader; epochs=10)\n",
    "    optim = Flux.setup(Flux.Adam(0.001), model) # 1e-3 usually safe starting LR\n",
    "    # optim = Flux.setup(Descent(0.001), model)\n",
    "\n",
    "    println(\"training on $(Threads.nthreads()) threads\")\n",
    "    flush(stdout)\n",
    "\n",
    "    for epoch in 1:epochs\n",
    "        batch_loss = 0.0\n",
    "        for (X_batch, y_batch) in train_loader\n",
    "\n",
    "            loss, grads = Flux.withgradient(model) do m\n",
    "                # loss = eval_loss_par(X_batch, y_batch, percent_error, m, Threads.nthreads())\n",
    "                loss = eval_loss(X_batch, y_batch, percent_error, m)\n",
    "                loss\n",
    "            end\n",
    "            batch_loss += loss\n",
    "            @assert !isnan(loss)\n",
    "\n",
    "            grads_fd = finite_diff_grads(model, X_batch, y_batch)\n",
    "            # @show grads[1]\n",
    "            # @show grads_fd      # Show FD gradients\n",
    "\n",
    "            Flux.update!(optim, model, grads[1])\n",
    "        end\n",
    "        batch_loss /= length(train_loader)\n",
    "        epoch % 1 == 0 && println(\"epoch $epoch: batch_loss = $batch_loss\")\n",
    "        flush(stdout)\n",
    "    end\n",
    "end\n",
    "\n",
    "function main(; epochs=15)\n",
    "    train_loader, test_loader = create_data(n_points=21, batch_size=7) # Should make 5 batches / epoch. 256 / 8 gives 32 evaluations per thread\n",
    "    @show n_features = length(first(train_loader)[1][1][1])\n",
    "\n",
    "    model = create_ff_model(n_features)\n",
    "    @show model.layers[1].weight, model([1.0])\n",
    "    train_model!(model, train_loader, test_loader; epochs=epochs)\n",
    "    return model\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.common_name = [\"n-butane\"]\n",
      "Generating data for 1 molecules...\n",
      "n_batches = 3, batch_size = 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_features = length((((first(train_loader))[1])[1])[1]) = 1\n",
      "training on 1 threads"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: batch_loss = 5.360681053341593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2: batch_loss = 5.095254476969392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3: batch_loss = 4.897990291885889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4: batch_loss = 4.716068695442626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5: batch_loss = 4.5406649955639145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6: batch_loss = 4.368539442838148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7: batch_loss = 4.198127423822174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8: batch_loss = 4.02855431249443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9: batch_loss = 3.8592855096452303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10: batch_loss = 3.689975341095186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11: batch_loss = 3.520392129478276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12: batch_loss = 3.3503771152134068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13: batch_loss = 3.1798206537278144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14: batch_loss = 3.0086468179930477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15: batch_loss = 2.8368034388344303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16: batch_loss = 2.6642561814038066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17: batch_loss = 2.4909829114810775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18: batch_loss = 2.316970810965202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19: batch_loss = 2.142213757826628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20: batch_loss = 1.9667107931264542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 21: batch_loss = 1.790464466729297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 22: batch_loss = 1.613480559258684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 23: batch_loss = 1.4357663158448926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 24: batch_loss = 1.2698875354190393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 25: batch_loss = 1.132949117650889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 26: batch_loss = 1.0274738021408802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 27: batch_loss = 0.9516407550856668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 28: batch_loss = 0.9003618821088573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 29: batch_loss = 0.8686134160576152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30: batch_loss = 0.8478455096292242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 31: batch_loss = 0.8350654164006324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 32: batch_loss = 0.8280746287812502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 33: batch_loss = 0.8252577543607602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 34: batch_loss = 0.8224166940669874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 35: batch_loss = 0.8173909195652144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 36: batch_loss = 0.8091421425004336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 37: batch_loss = 0.7983932960186033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 38: batch_loss = 0.7812298723085802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 39: batch_loss = 0.7553670525877793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 40: batch_loss = 0.7273135875886059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 41: batch_loss = 0.7017588832703124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 42: batch_loss = 0.6776388395646258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 43: batch_loss = 0.6544290014914161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 44: batch_loss = 0.6317444110134941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 45: batch_loss = 0.6093020702975974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 46: batch_loss = 0.5868938421666781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 47: batch_loss = 0.5643667161345426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 48: batch_loss = 0.5416079787954167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 49: batch_loss = 0.518535083296794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50: batch_loss = 0.4950869015724746\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Dense(1 => 3, #447; bias=false),      \u001b[90m# 3 parameters\u001b[39m\n",
       ") "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = main(;epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×1 Matrix{Float32}:\n",
       "  0.0070747477\n",
       " -0.02944446\n",
       "  0.12534449"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fieldnames(typeof(m.layers[1]))\n",
    "m.layers[1].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Float64}:\n",
       "   2.007074747700244\n",
       "   3.970555540174246\n",
       " 262.53444850444794"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "b = [2.0, 4.0, 250.0]\n",
    "c = Float64[1, 1, 100]\n",
    "m([1.0]) .* c .+ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.8514, 4.0887, 273.64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = CSV.read(\"./pcpsaft_params/SI_pcp-saft_parameters.csv\", DataFrame, header=1)\n",
    "filter!(row -> occursin(\"Alkane\", row.family), df)\n",
    "df = first(df, 1) #* Take only first molecule in dataframe\n",
    "mol_data = zip(df.common_name, df.isomeric_smiles, df.molarweight)\n",
    "saft_model = SAFTVRMie([first(mol_data)[1]])\n",
    "saft_model.params.segment.values[1], saft_model.params.sigma.values[1]*1e10, saft_model.params.epsilon.values[1]#, saft_model.params.lambda_a.values, saft_model.params.lambda_r.values, saft_model.params.epsilon.values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
