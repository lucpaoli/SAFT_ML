{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/SAFT_ML`\n"
     ]
    }
   ],
   "source": [
    "import Pkg; Pkg.activate(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: using Graphs.Δ in module Main conflicts with an existing identifier.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefinition of constant UNIT_FORMATS. This may fail, cause incorrect answers, or produce other errors.\n"
     ]
    }
   ],
   "source": [
    "using Clapeyron\n",
    "includet(\"./saftvrmienn.jl\")\n",
    "import Clapeyron: a_res\n",
    "\n",
    "using MolecularGraph, Graphs\n",
    "using Plots\n",
    "\n",
    "using Flux\n",
    "# using Flux: onecold, onehotbatch, logitcrossentropy\n",
    "using Flux: DataLoader\n",
    "using GraphNeuralNetworks\n",
    "using ForwardDiff, Zygote, ChainRulesCore\n",
    "\n",
    "using MLUtils\n",
    "using OneHotArrays\n",
    "# using LinearAlgebra, Random, Statistics\n",
    "using Statistics, Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# atom_symbol(mol): atom letters as a symbol e.g. :C, :O and :N\n",
    "# charge(mol): electric charge of the atom. only integer charge is allowed in the model\n",
    "# multiplicity(mol): 1: no unpaired electron(default), 2: radical, 3: biradical\n",
    "# lone_pair(mol): number of lone pair on the atom\n",
    "# implicit_hydrogens(mol): number of implicit hydrogens that are not appear as graph vertices but automatically calculated, drawn in image and used for calculation of other descriptors.\n",
    "# valence(mol): number of atom valence, specific to each atom species and considering electric charge. Implicit number of hydrogens is obtained by subtracting the degree of the vertex from the valence.\n",
    "# is_aromatic(mol): whether the atom is aromatic or not. only binary aromaticity is allowed in the model.\n",
    "# pi_electron(mol): number of pi electrons\n",
    "# hybridization(mol): orbital hybridization e.g. sp, sp2 and sp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GNNGraph:\n",
       "  num_nodes: 1\n",
       "  num_edges: 0\n",
       "  ndata:\n",
       "\tx = 11×1 Matrix{Float32}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function make_graph_from_smiles(smiles::String)\n",
    "    molgraph = smilestomol(smiles)\n",
    "\n",
    "    g = SimpleGraph(nv(molgraph))\n",
    "    for e in edges(molgraph)\n",
    "        add_edge!(g, e.src, e.dst)\n",
    "    end\n",
    "\n",
    "    # Should number of hydrogens be one-hot encoded?\n",
    "    f(vec, enc) = hcat(map(x -> onehot(x, enc), vec)...)\n",
    "    num_h = f(implicit_hydrogens(molgraph), [0, 1, 2, 3, 4])\n",
    "    hybrid = f(hybridization(molgraph), [:sp, :sp2, :sp3])\n",
    "    atoms = f(atom_symbol(molgraph), [:C, :O, :N])\n",
    "\n",
    "    # Node data should be matrix (num_features, num_nodes)\n",
    "    # Matrix has num_nodes columns, num_features rows\n",
    "    ndata = Float32.(vcat(num_h, hybrid, atoms))\n",
    "\n",
    "    # h(vec, enc) = hcat(map(x -> onehot(x, enc), vec)...)\n",
    "    # @show bond_order(molgraph), is_rotatable(molgraph), is_aromatic(molgraph), collect(edges(molgraph))\n",
    "    b_order = Float32.(f(bond_order(molgraph), [1, 2, 3]))\n",
    "    # @show b_order\n",
    "    # rotatable = f(is_rotatable(molgraph), [false, true])\n",
    "    # edata = Matrix{Float32}(vcat(b_order, rotatable))\n",
    "    # edata = Matrix{Float32}(b_order)\n",
    "    edata = nothing\n",
    "    \n",
    "    g = GNNGraph(g, ndata = ndata, edata = edata)\n",
    "    return g\n",
    "end\n",
    "\n",
    "g = make_graph_from_smiles(\"C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.04"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = SAFTVRMie([\"methane\"])\n",
    "fieldnames(typeof(model.params.Mw))\n",
    "model.params.Mw.values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: NaN found in V_vec at T = 627.610279365275 for decane\n",
      "└ @ Main /home/luc/SAFT_ML/4_gnn_saft.ipynb:69\n"
     ]
    }
   ],
   "source": [
    "# Iterate over molecules in dataset and build graph for each one\n",
    "# Initially sample data for hydrocarbons\n",
    "#! isobutane, isopentane not defined for SAFTVRMie\n",
    "all_species = [\n",
    "    \"methane\",\n",
    "    \"ethane\",\n",
    "    \"propane\",\n",
    "    \"butane\",\n",
    "    \"pentane\",\n",
    "    \"hexane\",\n",
    "    \"heptane\",\n",
    "    \"octane\",\n",
    "    \"nonane\",\n",
    "    \"decane\",\n",
    "]\n",
    "\n",
    "# Define smiles map\n",
    "smiles_map = Dict(\n",
    "    \"methane\" => \"C\",\n",
    "    \"ethane\" => \"CC\",\n",
    "    \"propane\" => \"CCC\",\n",
    "    \"butane\" => \"CCCC\",\n",
    "    \"isobutane\" => \"CC(C)C\",\n",
    "    \"pentane\" => \"CCCCC\",\n",
    "    \"isopentane\" => \"CC(C)CC\",\n",
    "    \"hexane\" => \"CCCCCC\",\n",
    "    \"heptane\" => \"CCCCCCC\",\n",
    "    \"octane\" => \"CCCCCCCC\",\n",
    "    \"nonane\" => \"CCCCCCCCC\",\n",
    "    \"decane\" => \"CCCCCCCCCC\",\n",
    ")\n",
    "\n",
    "# Create training data, currently sampled along saturation curve\n",
    "\n",
    "T = GNNGraph{Tuple{Vector{Int64}, Vector{Int64}, Nothing}}\n",
    "graphs = T[]\n",
    "states = Vector{Float32}[]\n",
    "species = String[] # For checking parameter similarity\n",
    "Y_data = Float32[]\n",
    "\n",
    "n = 15\n",
    "for s in all_species\n",
    "    # model = GERG2008([s])\n",
    "    model = SAFTVRMie([s])\n",
    "    Tc, pc, Vc = crit_pure(model)\n",
    "    smiles = smiles_map[s]\n",
    "\n",
    "    # fingerprint = make_fingerprint(smiles)\n",
    "    g = make_graph_from_smiles(smiles)\n",
    "\n",
    "    T_range = range(0.5 * Tc, 0.99 * Tc, n)\n",
    "    # V_range = range(0.5 * Vc, 1.5 * Vc, n) # V could be sampled from a logspace\n",
    "    for T in T_range\n",
    "        (p₀, V_vec...) = saturation_pressure(model, T)\n",
    "        if !any(isnan.(V_vec))\n",
    "            for V in V_vec\n",
    "                push!(graphs, g)\n",
    "                push!(species, s)\n",
    "\n",
    "                Mw = model.params.Mw.values[1]\n",
    "                m = model.params.segment.values[1]\n",
    "                push!(states, [V, T, Mw, m])\n",
    "\n",
    "                a = a_res(model, V, T, [1.0])\n",
    "                @assert !isnan(a) \"a is NaN at (V,T) = ($(V),$(T)) for $s\"\n",
    "                push!(Y_data, a)\n",
    "            end\n",
    "        else\n",
    "            @warn \"NaN found in V_vec at T = $T for $s\"\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(630.7620907026924, 2.2684498427389706e6, 0.0006551521374012365)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = SAFTgammaMie([\"decane\"])\n",
    "crit_pure(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GNNGraph:\n",
       "  num_nodes: 1\n",
       "  num_edges: 0\n",
       "  ndata:\n",
       "\tx = 11×1 Matrix{Float32}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "4-element Vector{Float32}:\n",
       "   0.0036027166\n",
       " 125.08929\n",
       "  16.04\n",
       "   1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "-0.058258507f0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 10\n",
    "display(graphs[n])\n",
    "display(states[n])\n",
    "display(Y_data[n])\n",
    "nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GNNGraph:\n",
       "  num_nodes: 162\n",
       "  num_edges: 260\n",
       "  num_graphs: 32\n",
       "  ndata:\n",
       "\tx = 11×162 Matrix{Float32}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data, test_data = splitobs((graphs, states, species, Y_data), at = 0.8, shuffle = true) |> getobs\n",
    "\n",
    "Random.seed!(0)\n",
    "train_loader = DataLoader(train_data, batchsize = 32, shuffle = true)\n",
    "test_loader = DataLoader(test_data, batchsize = 32, shuffle = false)\n",
    "\n",
    "# Testing if batching works. This will be used when training\n",
    "# This should produce a single GNNGraph object with a matrix of ndata\n",
    "vec_gs, _ = first(train_loader)\n",
    "MLUtils.batch(vec_gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "function differentiable_saft(X::AbstractVector{T}, Vol, Temp, Mw, m) where {T<:Real}\n",
    "    model = SAFTVRMieNN(\n",
    "        params=SAFTVRMieNNParams(\n",
    "            Mw=[Mw],\n",
    "            segment=[m], # (C - 4)/(3) + 1\n",
    "            sigma=[X[1]] * 1f-10,\n",
    "            lambda_a=[6.0], # Fixing at 6. Simple molecules interacting through London dispersion -> Should have λₐ = 6.\n",
    "            lambda_r=[X[2]],\n",
    "            epsilon=[X[3]],\n",
    "            # Required for association\n",
    "            epsilon_assoc=Float32[],\n",
    "            bondvol=Float32[],\n",
    "        )\n",
    "    )\n",
    "    return a_res(model, Vol, Temp, [1.0])\n",
    "end\n",
    "\n",
    "function ChainRulesCore.rrule(::typeof(differentiable_saft), x, V, T, Mw, m)\n",
    "    y = differentiable_saft(x, V, T, Mw, m)\n",
    "\n",
    "    function f_pullback(Δy)\n",
    "        # Use ForwardDiff to compute the gradient\n",
    "        #? ForwardDiff through nonlinear SAFT solvers not ideal.\n",
    "        ∂x = @thunk(ForwardDiff.gradient(x -> differentiable_saft(x, V, T, Mw, m), x) .* Δy)\n",
    "        ∂V = @thunk(ForwardDiff.derivative(V -> differentiable_saft(x, V, T, Mw, m), V) * Δy)\n",
    "        ∂T = @thunk(ForwardDiff.derivative(T -> differentiable_saft(x, V, T, Mw, m), T) * Δy)\n",
    "        ∂Mw = @thunk(ForwardDiff.derivative(Mw -> differentiable_saft(x, V, T, Mw, m), Mw) * Δy)\n",
    "        ∂m = @thunk(ForwardDiff.derivative(m -> differentiable_saft(x, V, T, Mw, m), m) * Δy)\n",
    "        return (NoTangent(), ∂x, ∂V, ∂T, ∂Mw, ∂m)\n",
    "    end\n",
    "\n",
    "    return y, f_pullback\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differentiable_saft(X, 0.0001, 300, 14.0, 1.0) = -0.2883850024854241\n",
      "ForwardDiff.gradient((x->begin\n",
      "            #= /home/luc/SAFT_ML/4_gnn_saft.ipynb:4 =#\n",
      "            differentiable_saft(x, 0.0001, 300, 14.0, 1.0)\n",
      "        end), X) = "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.12769665985165313, 0.053698346998990246, -0.0064937953559046635]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zygote.gradient((x->begin\n",
      "            #= /home/luc/SAFT_ML/4_gnn_saft.ipynb:5 =#\n",
      "            differentiable_saft(x, 0.0001, 300, 14.0, 1.0)\n",
      "        end), X) = ([-0.12769665985165313, 0.053698346998990246, -0.0064937953559046635],)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-0.12769665985165313, 0.053698346998990246, -0.0064937953559046635],)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#? Works for evaluation, fails for gradients\n",
    "X = [3.737, 12.504, 152.58]\n",
    "@show differentiable_saft(X, 1e-4, 300, 14.0, 1.0)\n",
    "@show ForwardDiff.gradient(x -> differentiable_saft(x, 1e-4, 300, 14.0, 1.0), X)\n",
    "@show Zygote.gradient(x -> differentiable_saft(x, 1e-4, 300, 14.0, 1.0), X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function create_graphconv_model(nin, nh; nout=3, nhlayers=1, afunc=relu)\n",
    "    GNNChain(\n",
    "        GraphConv(nin => nh, afunc),\n",
    "        [GraphConv(nh => nh, afunc) for _ in 1:nhlayers]...,\n",
    "        GlobalPool(mean), # Average the node features\n",
    "        Dropout(0.2),\n",
    "        Dense(nh, nh),\n",
    "        Dense(nh, nout),\n",
    "    )\n",
    "end\n",
    "\n",
    "function create_graphattention_model(nin, ein, nh; nout=3, nhlayers=1, afunc=relu)\n",
    "    GNNChain(\n",
    "        GATv2Conv((nin, ein) => nh, afunc),\n",
    "        [GATv2Conv(nh => nh, afunc) for _ in 1:nhlayers]...,\n",
    "        GlobalPool(mean),\n",
    "        Dropout(0.2),\n",
    "        Dense(nh, nh),\n",
    "        Dense(nh, nout),\n",
    "    )\n",
    "end\n",
    "\n",
    "function bound_output(X, lb, ub, b=10.0)\n",
    "    return @. lb + (ub - lb) * 0.5 * (tanh(1 / b * (X - lb) / (ub - lb)) + 1)\n",
    "end\n",
    "\n",
    "function predict_a_res(X, V, T, Mw, m)\n",
    "    # Bound output\n",
    "    bounds = Tuple{Float32,Float32}[\n",
    "        (2.5, 5), # σ\n",
    "        (10, 20), # λ_r\n",
    "        (100, 1000), # ϵ #? What should this be bounded to for SAFTVRMie?\n",
    "    ]\n",
    "    # ŷ = mean(X)\n",
    "    X_bounded = bound_output(X, first.(bounds), last.(bounds))\n",
    "    ŷ = differentiable_saft(X_bounded, V, T, Mw, m)\n",
    "    return ŷ + mean(X)\n",
    "end\n",
    "\n",
    "function eval_loss(model, data_loader, device)\n",
    "    loss = 0.0\n",
    "    acc = 0.0\n",
    "    for (g, state, species, y) in data_loader\n",
    "        g, state, y = MLUtils.batch(g) |> device, state |> device, y |> device\n",
    "        X = model(g, g.ndata.x)\n",
    "        for (Xᵢ, stateᵢ, yᵢ) in zip(eachcol(X), state, y)\n",
    "            V, T, Mw, m = stateᵢ\n",
    "            ŷ = predict_a_res(Xᵢ, V, T, Mw, m)\n",
    "            loss += ((ŷ - yᵢ) / yᵢ)^2\n",
    "            acc += abs((ŷ - yᵢ) / yᵢ)\n",
    "            @assert loss isa Real \"Loss is not a real number, got $(typeof(loss)), X_pred = $X_pred\"\n",
    "            @assert !isnan(loss) \"Loss is NaN, X_pred = $X_pred\"\n",
    "        end\n",
    "        loss /= length(state)\n",
    "        acc /= length(state)\n",
    "    end\n",
    "    loss /= length(data_loader)\n",
    "    acc /= length(data_loader)\n",
    "    # return loss, 100 * sqrt(loss)\n",
    "    return (loss = round(loss, digits = 4),\n",
    "            acc = round(100 * acc, digits = 2))\n",
    "end\n",
    "\n",
    "function train!(model; epochs=50, η=1e-2, infotime=10, log_loss=false)\n",
    "    # device = Flux.gpu # uncomment this for GPU training\n",
    "    device = Flux.cpu\n",
    "    model = model |> device\n",
    "    opt = ADAM()\n",
    "\n",
    "    function report(epoch)\n",
    "        train = eval_loss(model, train_loader, device)\n",
    "        test = eval_loss(model, test_loader, device)\n",
    "        @info (; epoch, train, test)\n",
    "    end\n",
    "\n",
    "    epoch_loss_vec = Float32[]\n",
    "    report(0)\n",
    "    for epoch in 1:epochs\n",
    "        epoch_loss = 0.0\n",
    "        for (g, state, species, y) in train_loader\n",
    "            g, state, y = MLUtils.batch(g) |> device, state |> device, y |> device\n",
    "\n",
    "            batch_loss = 0.0\n",
    "            loss_fn() = begin\n",
    "                X = model(g, g.ndata.x)\n",
    "                for (Xᵢ, stateᵢ, yᵢ) in zip(eachcol(X), state, y)\n",
    "                    V, T, Mw, m = stateᵢ\n",
    "                    ŷ = predict_a_res(Xᵢ, V, T, Mw, m)\n",
    "                    batch_loss += ((ŷ - yᵢ) / yᵢ)^2\n",
    "                    @assert batch_loss isa Real \"Loss is not a real number, got $(typeof(loss)), X_pred = $X_pred\"\n",
    "                    @assert !isnan(batch_loss) \"Loss is NaN, X_pred = $X_pred\"\n",
    "                end\n",
    "                batch_loss /= length(state)\n",
    "            end\n",
    "\n",
    "            grads = Zygote.gradient(Flux.params(model)) do\n",
    "                loss_fn()\n",
    "            end\n",
    "            epoch_loss += batch_loss\n",
    "            Flux.update!(opt, Flux.params(model), grads)\n",
    "        end\n",
    "        epoch_loss /= length(train_loader)\n",
    "        push!(epoch_loss_vec, epoch_loss)\n",
    "        \n",
    "        epoch % infotime == 0 && report(epoch)\n",
    "    end\n",
    "    return epoch_loss_vec\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: nh = 16\n",
      "└ @ Main /home/luc/SAFT_ML/4_gnn_saft.ipynb:3\n",
      "┌ Info: (epoch = 0, train = (loss = 67.5765, acc = 108.01), test = (loss = 169.113, acc = 335.32))\n",
      "└ @ Main /home/luc/SAFT_ML/4_gnn_saft.ipynb:72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: (epoch = 50, train = (loss = 2.2443, acc = 43.53), test = (loss = 6.4945, acc = 131.55))\n",
      "└ @ Main /home/luc/SAFT_ML/4_gnn_saft.ipynb:72\n",
      "┌ Info: nh = 64\n",
      "└ @ Main /home/luc/SAFT_ML/4_gnn_saft.ipynb:3\n",
      "┌ Info: (epoch = 0, train = (loss = 26.691, acc = 87.3), test = (loss = 89.645, acc = 310.24))\n",
      "└ @ Main /home/luc/SAFT_ML/4_gnn_saft.ipynb:72\n",
      "┌ Info: (epoch = 50, train = (loss = 1.17, acc = 29.72), test = (loss = 3.9623, acc = 110.99))\n",
      "└ @ Main /home/luc/SAFT_ML/4_gnn_saft.ipynb:72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: nh = 128\n",
      "└ @ Main /home/luc/SAFT_ML/4_gnn_saft.ipynb:3\n",
      "┌ Info: (epoch = 0, train = (loss = 210.3582, acc = 210.27), test = (loss = 579.042, acc = 528.27))\n",
      "└ @ Main /home/luc/SAFT_ML/4_gnn_saft.ipynb:72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: (epoch = 50, train = (loss = 1.4218, acc = 32.77), test = (loss = 11.0201, acc = 149.28))\n",
      "└ @ Main /home/luc/SAFT_ML/4_gnn_saft.ipynb:72\n",
      "┌ Info: nh = 256\n",
      "└ @ Main /home/luc/SAFT_ML/4_gnn_saft.ipynb:3\n",
      "┌ Info: (epoch = 0, train = (loss = 10.1809, acc = 46.13), test = (loss = 20.1314, acc = 160.87))\n",
      "└ @ Main /home/luc/SAFT_ML/4_gnn_saft.ipynb:72\n",
      "┌ Info: (epoch = 50, train = (loss = 1.3892, acc = 26.78), test = (loss = 16.9619, acc = 143.71))\n",
      "└ @ Main /home/luc/SAFT_ML/4_gnn_saft.ipynb:72\n"
     ]
    }
   ],
   "source": [
    "nin = 11\n",
    "for nh ∈ [16, 64, 128, 256]\n",
    "    @info \"nh = $nh\"\n",
    "    model = create_graphattention_model(nin, nh, nhlayers=1)\n",
    "    epoch_loss_vec = train!(model, epochs=50, infotime=50)\n",
    "end\n",
    "# nh = 16\n",
    "# model = create_graphattention_model(nin, nh, nhlayers=1)\n",
    "# model = create_graphconv_model(nin, nh; nhlayers=1)\n",
    "# epoch_loss_vec = train!(model, epochs=50, infotime=50)\n",
    "nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: nh = 16\n",
      "└ @ Main /home/luc/SAFT_ML/4_gnn_saft.ipynb:3\n",
      "┌ Info: (epoch = 0, train = (loss = 17661.8113, acc = 1517.87), test = (loss = 9717.8597, acc = 2010.68))\n",
      "└ @ Main /home/luc/SAFT_ML/4_gnn_saft.ipynb:72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: (epoch = 50, train = (loss = 9.6704, acc = 56.25), test = (loss = 53.5042, acc = 200.69))\n",
      "└ @ Main /home/luc/SAFT_ML/4_gnn_saft.ipynb:72\n",
      "┌ Info: nh = 64\n",
      "└ @ Main /home/luc/SAFT_ML/4_gnn_saft.ipynb:3\n",
      "┌ Info: (epoch = 0, train = (loss = 14.7587, acc = 58.99), test = (loss = 50.9815, acc = 226.55))\n",
      "└ @ Main /home/luc/SAFT_ML/4_gnn_saft.ipynb:72\n",
      "┌ Info: (epoch = 50, train = (loss = 11.2679, acc = 38.86), test = (loss = 64.9709, acc = 244.51))\n",
      "└ @ Main /home/luc/SAFT_ML/4_gnn_saft.ipynb:72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: nh = 128\n",
      "└ @ Main /home/luc/SAFT_ML/4_gnn_saft.ipynb:3\n",
      "┌ Info: (epoch = 0, train = (loss = 2.8101, acc = 36.28), test = (loss = 15.331, acc = 152.46))\n",
      "└ @ Main /home/luc/SAFT_ML/4_gnn_saft.ipynb:72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: (epoch = 50, train = (loss = 1.6628, acc = 22.24), test = (loss = 13.1399, acc = 136.3))\n",
      "└ @ Main /home/luc/SAFT_ML/4_gnn_saft.ipynb:72\n",
      "┌ Info: nh = 256\n",
      "└ @ Main /home/luc/SAFT_ML/4_gnn_saft.ipynb:3\n",
      "┌ Info: (epoch = 0, train = (loss = 28.203, acc = 67.42), test = (loss = 1460.2671, acc = 809.78))\n",
      "└ @ Main /home/luc/SAFT_ML/4_gnn_saft.ipynb:72\n",
      "┌ Info: (epoch = 50, train = (loss = 2.8036, acc = 33.82), test = (loss = 4.8567, acc = 114.86))\n",
      "└ @ Main /home/luc/SAFT_ML/4_gnn_saft.ipynb:72\n"
     ]
    }
   ],
   "source": [
    "nin = 11\n",
    "for nh ∈ [16, 64, 128, 256]\n",
    "    @info \"nh = $nh\"\n",
    "    model = create_graphconv_model(nin, nh; nhlayers=1)\n",
    "    epoch_loss_vec = train!(model, epochs=50, infotime=50)\n",
    "end\n",
    "nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Float64}:\n",
       "   3.620209658064277\n",
       "  13.453103363749996\n",
       " 217.57003215248915"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3-element Vector{Float64}:\n",
       "   4.0887\n",
       "  13.65\n",
       " 273.64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(first(state), first(species), first(y)) = (Float32[9.589684f-5, 267.38336, 58.12, 1.8514], \"butane\", -4.662166f0)\n"
     ]
    }
   ],
   "source": [
    "# Abstract this into a function that takes a model and returns the X_bounded output\n",
    "g, state, species, y = first(train_loader)\n",
    "g = MLUtils.batch(g)\n",
    "X = model(g, g.ndata.x)\n",
    "X = first(eachcol(X))\n",
    "@show first(state), first(species), first(y)\n",
    "bounds = Tuple{Float32,Float32}[\n",
    "    (2.5, 5),\n",
    "    (10, 18),\n",
    "    (150, 300),\n",
    "]\n",
    "# ŷ = mean(X)\n",
    "X_bounded = bound_output(X, first.(bounds), last.(bounds))\n",
    "m = SAFTVRMie([first(species)])\n",
    "X_nominal = [m.params.sigma.values[1]*1e10, m.params.lambda_r.values[1], m.params.epsilon.values[1]]\n",
    "display(X_bounded)\n",
    "display(X_nominal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Float64}:\n",
       "   3.737e-10\n",
       "  12.504\n",
       " 152.58"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = SAFTVRMie([\"methane\"])\n",
    "X_nominal = [m.params.sigma.values[1], m.params.lambda_r.values[1], m.params.epsilon.values[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: epoch_loss_vec not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: epoch_loss_vec not defined\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/SAFT_ML/4_gnn_saft.ipynb:1"
     ]
    }
   ],
   "source": [
    "plot(epoch_loss_vec, label = \"Training loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: epoch_loss_vec not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: epoch_loss_vec not defined\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/SAFT_ML/4_gnn_saft.ipynb:1"
     ]
    }
   ],
   "source": [
    "plot(epoch_loss_vec, label = \"Training loss\", ylims=(0.0, 0.02))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next:\n",
    "# - Take notebooks, extract to .jl single-functions and run hyperparameter sweep\n",
    "# - Evaluate AAD on saturation pressure\n",
    "# - Stratify test/train data by molecule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
